{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7cdc0oJjb8N5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Michael\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\numpy\\\\core\\\\_multiarray_tests.cp312-win_amd64.pyd'\n",
            "Check the permissions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install --ignore-install -q pyarrow\n",
        "!pip install --ignore-install -q pyspark\n",
        "!pip install --ignore-install -q findspark\n",
        "!pip install --ignore-install -q yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geYImRvXMt3O",
        "outputId": "23635c73-30d7-45dd-91f1-7e9df6e3ba86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'test'...\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  !git clone https://github.com/Michaelwwk/test.git\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pyarrow.csv as pv\n",
        "import pyarrow.parquet as pq\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# import collections\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "# from google.colab import drive\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DateType\n",
        "from datetime import datetime\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.types import IntegerType\n",
        "import pyspark.sql.functions as func\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# on Local\n",
        "\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Stock Ticker\").config('spark.ui.port', '4050').getOrCreate()\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "someFile = \"wordcount.txt\"\n",
        "stock_folder = \"StockData\"\n",
        "stocks = spark.read.csv(stock_folder, header=True)\n",
        "peopleDF = spark.read.json(\"people.json\")\n",
        "\n",
        "df_csv = spark.read.format('csv') \\\n",
        "                .option(\"inferSchema\",\"true\") \\\n",
        "                .option(\"header\",\"true\") \\\n",
        "                .option(\"sep\",\";\") \\\n",
        "                .load(\"people.csv\")\n",
        "\n",
        "df_json = spark.read.format('json') \\\n",
        "                .option(\"inferSchema\",\"true\") \\\n",
        "                .option(\"header\",\"true\") \\\n",
        "                .option(\"sep\",\";\") \\\n",
        "                .load(\"people.json\")\n",
        "\n",
        "# read and convert hdb resale price\n",
        "hdb_table = pv.read_csv(\"resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.csv\")\n",
        "iris_data = spark.read.csv(\"iris-data.csv\", header=True, inferSchema=True)\n",
        "pq.write_table(hdb_table,'resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.parquet')\n",
        "hdb_parquet = pq.ParquetFile('resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IrvWniDyMrdd"
      },
      "outputs": [],
      "source": [
        "# # on Colab\n",
        "\n",
        "# spark = SparkSession.builder.master(\"local\").appName(\"Stock Ticker\").config('spark.ui.port', '4050').getOrCreate()\n",
        "# # drive.mount('/content/drive')\n",
        "\n",
        "# someFile = \"/content/test/wordcount.txt\"\n",
        "# stock_folder = \"/content/test/StockData\"\n",
        "# stocks = spark.read.csv(stock_folder, header=True)\n",
        "# peopleDF = spark.read.json(\"/content/test/people.json\")\n",
        "\n",
        "# df_csv = spark.read.format('csv') \\\n",
        "#                 .option(\"inferSchema\",\"true\") \\\n",
        "#                 .option(\"header\",\"true\") \\\n",
        "#                 .option(\"sep\",\";\") \\\n",
        "#                 .load(\"/content/test/people.csv\")\n",
        "\n",
        "# df_json = spark.read.format('json') \\\n",
        "#                 .option(\"inferSchema\",\"true\") \\\n",
        "#                 .option(\"header\",\"true\") \\\n",
        "#                 .option(\"sep\",\";\") \\\n",
        "#                 .load(\"/content/test/people.json\")\n",
        "\n",
        "# # read and convert hdb resale price\n",
        "# hdb_table = pv.read_csv(\"/content/test/resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.csv\")\n",
        "# iris_data = spark.read.csv(\"/content/drive/MyDrive/iris-data.csv\", header=True, inferSchema=True)\n",
        "# pq.write_table(hdb_table,'/content/test/resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.parquet')\n",
        "# hdb_parquet = pq.ParquetFile('/content/test/resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l1I1VynS4JBT"
      },
      "outputs": [],
      "source": [
        "# import collections\n",
        "# spark = SparkSession.builder.master(\"local\").appName(\"My App \").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "c-XvTqWhANe5",
        "outputId": "e5cf809c-dafc-44d4-a14d-e3aefddf6a0f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://Michael-ASUS-ZenBook:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Stock Ticker</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local appName=Stock Ticker>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRm36sJXv7mr",
        "outputId": "a859240b-8123-4a81-fab5-bb4dd3264a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20\n"
          ]
        }
      ],
      "source": [
        "# the above file is under your pythonProject folder\n",
        "spark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n",
        "print(spark.read.text(someFile).count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ddxRp3RNh1Fx"
      },
      "outputs": [],
      "source": [
        "# to read in data from a text file, first upload the data file into your google drive and then mount your google drive onto colab\n",
        "# to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True)\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3o9yPBOr-ss",
        "outputId": "787daac1-fc46-43d3-b9c4-2ef58a030f2a"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 7) (Michael-ASUS-ZenBook executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.4.2-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 683, in main\nRuntimeError: Python in worker has different version 3.10 than that in driver 3.12, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.4.2-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 683, in main\nRuntimeError: Python in worker has different version 3.10 than that in driver 3.12, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m rdd \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mparallelize((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Traditional Python map(function, collection) (few MBs - GB fails)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Scalabale map (Peta - support)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# iterablecollection.map(function) -> Object\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# collect() Object to collection\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtuple\u001b[39m(\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 7) (Michael-ASUS-ZenBook executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.4.2-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 683, in main\nRuntimeError: Python in worker has different version 3.10 than that in driver 3.12, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.4.2-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 683, in main\nRuntimeError: Python in worker has different version 3.10 than that in driver 3.12, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n"
          ]
        }
      ],
      "source": [
        "rdd = spark.sparkContext.parallelize((1, 2, 3, 4, 5))\n",
        "# Traditional Python map(function, collection) (few MBs - GB fails)\n",
        "# Scalabale map (Peta - support)\n",
        "# iterablecollection.map(function) -> Object\n",
        "# collect() Object to collection\n",
        "print(tuple(rdd.map(lambda x: x * x).collect()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJKFicO-sSrR",
        "outputId": "6f70845b-62e0-40fe-910e-aff0dea5cbac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 4]\n"
          ]
        }
      ],
      "source": [
        "print(rdd.filter(lambda x: x % 2 == 0).collect())  # Keeps even numbers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BrAp7Uwskim",
        "outputId": "d2d05981-3261-4447-eb46-bb53f262f8e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hello', 'world', 'hi', 'hello', 'mars', 'hello', 'jupiter', 'hello', 'saturn']\n"
          ]
        }
      ],
      "source": [
        "words = spark.sparkContext.parallelize([\"hello world\", \"hi\", \"hello mars\", \"hello jupiter\", \"hello saturn\"])\n",
        "print(words.flatMap(lambda x: x.split(\" \")).collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74xIHlGchlEE",
        "outputId": "02100066-a869-458f-ffd2-d3c69808b802"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hello', 'world', 'hi', 'mars', 'jupiter', 'saturn']\n"
          ]
        }
      ],
      "source": [
        "print(words.flatMap(lambda x: x.split(\" \")).distinct().collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGk2e8kLs6cr",
        "outputId": "30a2c9ac-c034-4c06-dce2-583cb2a38528"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4]\n"
          ]
        }
      ],
      "source": [
        "tuples = spark.sparkContext.parallelize((1, 1, 2, 3, 3, 4))\n",
        "print(tuples.distinct().collect())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrJG5bFCtaIy",
        "outputId": "97078203-8bfd-4c53-f5e0-33004502185f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4]\n"
          ]
        }
      ],
      "source": [
        "print(tuples.distinct().collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yi1FyZRtlcn",
        "outputId": "b313de4b-8013-471e-9041-4f7de8982fb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "print(tuples.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_EERBI9tuQu",
        "outputId": "20b04c46-af9c-4e08-9606-89976508cb94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1, 2]\n"
          ]
        }
      ],
      "source": [
        "first_three = tuples.take(3)\n",
        "print(first_three)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY7SkGfvt2-g",
        "outputId": "7b128479-58b5-4faa-ffe2-ad5e7cdfbc5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14\n"
          ]
        }
      ],
      "source": [
        "sum = tuples.reduce(lambda a, b: a + b)\n",
        "print(sum)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y1ezBHTOrAf",
        "outputId": "504332f9-d24e-4575-d181-6bb0cec1c93d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretty printing has been turned ON\n"
          ]
        }
      ],
      "source": [
        "pprint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf_-cHMhvD0V",
        "outputId": "8f53bf6a-273b-442d-9e93-e2501b8e7bae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3: (6, 4)\n",
            "1: (2,)\n",
            "[(3, 10), (1, 2)]\n",
            "[(1, 2), (3, 6), (3, 4)]\n"
          ]
        }
      ],
      "source": [
        "rdd = spark.sparkContext.parallelize([(3, 6),(1, 2),(3, 4)])\n",
        "grouped = rdd.groupByKey()\n",
        "for key, values in grouped.collect():\n",
        "    print(f\"{key}: {tuple(values)}\")\n",
        "reduced = rdd.reduceByKey(lambda a, b: a + b)\n",
        "print(reduced.collect())\n",
        "sorted_rdd = rdd.sortByKey()\n",
        "print(sorted_rdd.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr-ClfANyEsJ",
        "outputId": "f1f146b4-449a-476e-8151-fa9f29c1d7d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Alice', 1), ('Bob', 2), ('Charlie', 3), ('David', 4)]\n"
          ]
        }
      ],
      "source": [
        "# Create two RDDs\n",
        "rdd1 = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2)])\n",
        "rdd2 = spark.sparkContext.parallelize([(\"Charlie\", 3), (\"David\", 4)])\n",
        "\n",
        "# Perform the union operation\n",
        "union_rdd = rdd1.union(rdd2)\n",
        "\n",
        "# Collect and print the results\n",
        "print(union_rdd.collect())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsgtMDGHyVK6",
        "outputId": "4ccbb7a3-d160-4b77-c1b0-edabd51da4c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Alice', ('Apple', 1)), ('Bob', ('Banana', 2))]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create two RDDs with common keys\n",
        "rdd3 = spark.sparkContext.parallelize([(\"Alice\", \"Apple\"), (\"Bob\", \"Banana\")])\n",
        "rdd4 = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2)])\n",
        "\n",
        "# Perform the join operation\n",
        "join_rdd = rdd3.join(rdd4)\n",
        "\n",
        "# Collect and print the results\n",
        "print(join_rdd.collect())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw643DrQypKe",
        "outputId": "4f4e3fcc-d856-42d8-d730-5bd8c3959cb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')]\n"
          ]
        }
      ],
      "source": [
        "# Create two RDDs\n",
        "rdd5 = spark.sparkContext.parallelize([1, 2])\n",
        "rdd6 = spark.sparkContext.parallelize([\"a\", \"b\"])\n",
        "\n",
        "# Perform the cartesian operation\n",
        "cross_rdd = rdd5.cartesian(rdd6)\n",
        "\n",
        "# Collect and print the results\n",
        "print(cross_rdd.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfi1TC6nMrdk"
      },
      "outputs": [],
      "source": [
        "# # to read in data from a text file, first upload the data file into your google drive and then mount your google drive onto colab\n",
        "# from google.colab import drive\n",
        "# # to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True)\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKBmBggQMrdl"
      },
      "outputs": [],
      "source": [
        "# # install pyspark using pip\n",
        "# !pip install --ignore-install -q pyspark\n",
        "# # install findspark using pip\n",
        "# !pip install --ignore-install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWBh0lLCMrdl"
      },
      "outputs": [],
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "# import yfinance as yf\n",
        "# stock_folder = \"/content/drive/MyDrive/data/StockData\"\n",
        "# # stock_prices_data.to_csv(\"aapl_stock_prices_data.csv\")\n",
        "\n",
        "# from pyspark.sql.functions import udf\n",
        "# from pyspark.sql.types import DateType\n",
        "# from datetime import datetime\n",
        "\n",
        "# from pyspark.sql.types import FloatType\n",
        "# from pyspark.sql.types import IntegerType\n",
        "# import pyspark.sql.functions as func\n",
        "# from pyspark.sql.window import Window\n",
        "\n",
        "# ## Reading CSV data => Stocks\n",
        "# stocks = spark.read.csv(stock_folder, header=True)\n",
        "\n",
        "# # import collections\n",
        "# spark = SparkSession.builder.master(\"local\").appName(\"Stock Ticker\").config('spark.ui.port', '4050').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We3RDwlCMrdl",
        "outputId": "f6b183c5-b676-4f55-83bb-943ae3e60f63"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "\n",
        "ticker = \"AAPL\"  # Replace with the ticker symbol of the company you want to analyze\n",
        "start_date = \"2020-01-01\"\n",
        "end_date = \"2023-12-28\"\n",
        "\n",
        "stock_prices_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "\n",
        "stock_prices_data.to_csv(\"aapl_stock_prices_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ud2GO1FzMrdl",
        "outputId": "d36232fc-0b2b-477d-af91-1bf35caa77d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+----------+-------+--------+--------+--------+\n",
            "|Ticker|      Date|Close/Last| Volume|    Open|    High|     Low|\n",
            "+------+----------+----------+-------+--------+--------+--------+\n",
            "| BRK-B|05/31/2023|  $321.08 |6175417|$321.12 |$322.41 |$319.39 |\n",
            "| BRK-B|05/30/2023|  $322.19 |3232461|$321.86 |$322.47 |$319.00 |\n",
            "| BRK-B|05/26/2023|  $320.60 |3229873|$320.44 |$322.63 |$319.67 |\n",
            "| BRK-B|05/25/2023|  $319.02 |4251935|$320.56 |$320.56 |$317.71 |\n",
            "| BRK-B|05/24/2023|  $320.20 |3075393|$322.71 |$323.00 |$319.56 |\n",
            "+------+----------+----------+-------+--------+--------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Seeing Data => Dataframe\n",
        "stocks.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1uZrJADMrdm",
        "outputId": "e85db877-812e-4d22-894e-6613fc7896af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Ticker: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Close/Last: string (nullable = true)\n",
            " |-- Volume: string (nullable = true)\n",
            " |-- Open: string (nullable = true)\n",
            " |-- High: string (nullable = true)\n",
            " |-- Low: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Seeing Schema of the Data => Data Types in Dataframe\n",
        "stocks.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z9YNpa6Mrdm",
        "outputId": "f6c7d9f2-9ce8-45e1-fc58-d15ddac0e838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+\n",
            "|Ticker|\n",
            "+------+\n",
            "| BRK-B|\n",
            "| BRK-B|\n",
            "| BRK-B|\n",
            "+------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Basic select operation => Select Ticker, Date and Close price\n",
        "stocks.select(\"Ticker\").show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JxDF4lEMrdm",
        "outputId": "76598508-ee6c-4108-db36-46035fece462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+--------+\n",
            "|Ticker|      Date|    Open|\n",
            "+------+----------+--------+\n",
            "| BRK-B|05/31/2023|$321.12 |\n",
            "| BRK-B|05/30/2023|$321.86 |\n",
            "| BRK-B|05/26/2023|$320.44 |\n",
            "| BRK-B|05/25/2023|$320.56 |\n",
            "| BRK-B|05/24/2023|$322.71 |\n",
            "+------+----------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stocks.select([\"Ticker\", \"Date\", \"Open\"]).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlZbuozYMrdm",
        "outputId": "62d06243-5cae-482e-be58-07380e61bdf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+----------+--------+--------+--------+--------+\n",
            "|Ticker|      Date|Close/Last|  Volume|    Open|    High|     Low|\n",
            "+------+----------+----------+--------+--------+--------+--------+\n",
            "|  MSFT|05/31/2023|  $328.39 |45950550|$332.29 |$335.94 |$327.33 |\n",
            "|  MSFT|05/30/2023|  $331.21 |29503070|$335.23 |$335.74 |$330.52 |\n",
            "|  MSFT|05/26/2023|  $332.89 |36630630|$324.02 |$333.40 |$323.88 |\n",
            "|  MSFT|05/25/2023|  $325.92 |43301740|$323.24 |$326.90 |$320.00 |\n",
            "|  MSFT|05/24/2023|  $313.85 |23384890|$314.73 |$316.50 |$312.61 |\n",
            "|  MSFT|05/23/2023|  $315.26 |30797170|$320.03 |$322.72 |$315.25 |\n",
            "|  MSFT|05/22/2023|  $321.18 |24115660|$318.60 |$322.59 |$318.01 |\n",
            "|  MSFT|05/19/2023|  $318.34 |27546700|$316.74 |$318.75 |$316.37 |\n",
            "|  MSFT|05/18/2023|  $318.52 |27275990|$314.53 |$319.04 |$313.72 |\n",
            "|  MSFT|05/17/2023|  $314.00 |24315010|$312.29 |$314.43 |$310.74 |\n",
            "+------+----------+----------+--------+--------+--------+--------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Filtering Data => Select rows containing Microsoft Stock in last one month\n",
        "stocks.filter(stocks.Ticker == \"MSFT\").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K1hvYbSMrdm",
        "outputId": "40938417-191c-4996-f1e2-897e6dad28d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+----------+--------+--------+--------+--------+\n",
            "|Ticker|      Date|Close/Last|  Volume|    Open|    High|     Low|\n",
            "+------+----------+----------+--------+--------+--------+--------+\n",
            "|  MSFT|05/31/2023|  $328.39 |45950550|$332.29 |$335.94 |$327.33 |\n",
            "+------+----------+----------+--------+--------+--------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stocks.filter((stocks.Ticker == \"MSFT\") & (stocks.Date == \"05/31/2023\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFBA2meuMrdm",
        "outputId": "56c1891b-91ac-4b25-daff-82a692db5ebf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+----------+--------+--------+--------+--------+\n",
            "|Ticker|      Date|Close/Last|  Volume|    Open|    High|     Low|\n",
            "+------+----------+----------+--------+--------+--------+--------+\n",
            "|  MSFT|05/31/2023|  $328.39 |45950550|$332.29 |$335.94 |$327.33 |\n",
            "|     V|05/31/2023|  $221.03 |20460620|$219.96 |$221.53 |$216.14 |\n",
            "+------+----------+----------+--------+--------+--------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stocks.filter(((stocks.Ticker == \"MSFT\") | (stocks.Ticker == \"V\")) & (stocks.Date == \"05/31/2023\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s9owfWRMrdm",
        "outputId": "7aa07536-a4fe-404e-b7a1-010d43102821"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+----------+---------+--------+--------+--------+\n",
            "|Ticker|      Date|Close/Last|   Volume|    Open|    High|     Low|\n",
            "+------+----------+----------+---------+--------+--------+--------+\n",
            "|  MSFT|05/31/2023|  $328.39 | 45950550|$332.29 |$335.94 |$327.33 |\n",
            "|  TSLA|05/31/2023|  $203.93 |150711700|$199.78 |$203.95 |$195.12 |\n",
            "|     V|05/31/2023|  $221.03 | 20460620|$219.96 |$221.53 |$216.14 |\n",
            "|   SPY|05/31/2023|    417.85|110811800|  418.28|  419.22|  416.22|\n",
            "|   QQQ|05/31/2023|    347.99| 65105380|  348.37|   350.6|  346.51|\n",
            "+------+----------+----------+---------+--------+--------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stocks.filter((stocks.Ticker.isin([\"MSFT\", \"QQQ\", \"SPY\", \"V\", \"TSLA\"])) & (stocks.Date == \"05/31/2023\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSBBShurMrdn",
        "outputId": "df87d523-f70a-451d-e71d-ad1bd3084e9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Ticker: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Close/Last: string (nullable = true)\n",
            " |-- Volume: string (nullable = true)\n",
            " |-- Open: string (nullable = true)\n",
            " |-- High: string (nullable = true)\n",
            " |-- Low: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stocks.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDmchJmBMrdn",
        "outputId": "9f48c38e-74da-4735-c69a-0853f0dc9b80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Ticker: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Close/Last: string (nullable = true)\n",
            " |-- Volume: string (nullable = true)\n",
            " |-- Open: string (nullable = true)\n",
            " |-- High: string (nullable = true)\n",
            " |-- Low: string (nullable = true)\n",
            " |-- ParsedDate: date (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## User Defined Functions\n",
        "\n",
        "date_parser = udf(lambda date: datetime.strptime(date,\"%m/%d/%Y\"), DateType())\n",
        "stocks = stocks.withColumn(\"ParsedDate\", date_parser(stocks.Date))\n",
        "stocks.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eHP3e55Mrdn"
      },
      "outputs": [],
      "source": [
        "def num_parser(value):\n",
        "    if isinstance(value, str):\n",
        "        return float(value.strip(\"$\"))\n",
        "    elif isinstance(value, int) or isinstance(value, float):\n",
        "        return value\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "parser_number = udf(num_parser, FloatType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3APhDQvsMrdn"
      },
      "outputs": [],
      "source": [
        "stocks = (stocks.withColumn(\"Open\", parser_number(stocks.Open))\n",
        "                .withColumn(\"Close\", parser_number(stocks[\"Close/Last\"]))\n",
        "                .withColumn(\"Low\", parser_number(stocks.Low))\n",
        "                .withColumn(\"High\", parser_number(stocks.High)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5QDPWDXMrdn",
        "outputId": "b184720a-cc9d-49e2-aa09-af3b6d657eca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Ticker: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Close/Last: string (nullable = true)\n",
            " |-- Volume: string (nullable = true)\n",
            " |-- Open: float (nullable = true)\n",
            " |-- High: float (nullable = true)\n",
            " |-- Low: float (nullable = true)\n",
            " |-- ParsedDate: date (nullable = true)\n",
            " |-- Close: float (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stocks.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcV4f37AMrdn"
      },
      "outputs": [],
      "source": [
        "parse_int = udf(lambda value: int(value), IntegerType())\n",
        "## Changing the datatype of the column\n",
        "stocks = stocks.withColumn(\"Volume\", parse_int(stocks.Volume))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUhgMYzHMrdn",
        "outputId": "4d3f57ce-3972-4e85-a822-14902da831af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Ticker: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Close/Last: string (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Open: float (nullable = true)\n",
            " |-- High: float (nullable = true)\n",
            " |-- Low: float (nullable = true)\n",
            " |-- ParsedDate: date (nullable = true)\n",
            " |-- Close: float (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stocks.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwFzLncZMrdo",
        "outputId": "ba586ebd-5bf9-4661-cce8-95b72cedfc61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+-------+------+------+------+------+\n",
            "|Ticker|ParsedDate| Volume|  Open|   Low|  High| Close|\n",
            "+------+----------+-------+------+------+------+------+\n",
            "| BRK-B|2023-05-31|6175417|321.12|319.39|322.41|321.08|\n",
            "| BRK-B|2023-05-30|3232461|321.86| 319.0|322.47|322.19|\n",
            "| BRK-B|2023-05-26|3229873|320.44|319.67|322.63| 320.6|\n",
            "| BRK-B|2023-05-25|4251935|320.56|317.71|320.56|319.02|\n",
            "| BRK-B|2023-05-24|3075393|322.71|319.56| 323.0| 320.2|\n",
            "+------+----------+-------+------+------+------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cleaned_stocks = stocks.select([\"Ticker\", \"ParsedDate\", \"Volume\", \"Open\", \"Low\", \"High\", \"Close\"])\n",
        "cleaned_stocks.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPrTs8FaMrdo",
        "outputId": "6fc9c08d-6c5e-4dec-db1a-a12e5bb3e0ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------------------+------------------+------------------+------------------+------------------+\n",
            "|summary|              Volume|              Open|               Low|              High|             Close|\n",
            "+-------+--------------------+------------------+------------------+------------------+------------------+\n",
            "|  count|               15108|             15108|             15108|             15108|             15108|\n",
            "|   mean|5.1868408793685466E7|180.09656566181036| 177.9982781513109| 182.1253348687101| 180.1256089860054|\n",
            "| stddev| 5.496484129953461E7|101.16125813324406|100.26590135955226|101.96625521621772|101.14891782168523|\n",
            "|    min|              961133|             12.07|              11.8|             12.45|             11.93|\n",
            "|    max|           914080943|            479.22|            476.06|            479.98|            477.71|\n",
            "+-------+--------------------+------------------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Calculating basic stastics about data => Calculate average stock price\n",
        "cleaned_stocks.describe([\"Volume\", \"Open\", \"Low\", \"High\", \"Close\"]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15ImyQY1Mrdo",
        "outputId": "59c49174-2bde-4b2f-92fe-0e4cf1dd0ff9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+---------+\n",
            "|Ticker|max(Open)|\n",
            "+------+---------+\n",
            "| BRK-B|   361.39|\n",
            "|   TSM|   141.61|\n",
            "|  AAPL|   182.63|\n",
            "|  META|   381.68|\n",
            "|  TSLA|   411.47|\n",
            "|   QQQ|   405.57|\n",
            "|     V|   250.05|\n",
            "| GOOGL|   151.25|\n",
            "|   SPY|   479.22|\n",
            "|  AMZN|    187.2|\n",
            "|  MSFT|   344.62|\n",
            "|  NVDA|   405.95|\n",
            "+------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Calculate maximum stock price for various stocks\n",
        "cleaned_stocks.groupBy(\"Ticker\").max(\"Open\").show(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bObrPFWWMrdo",
        "outputId": "b040d352-ae25-4ace-c318-f25cc313b0ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------------+\n",
            "|Ticker|MaxStockPrice|\n",
            "+------+-------------+\n",
            "| BRK-B|       361.39|\n",
            "|   TSM|       141.61|\n",
            "|  AAPL|       182.63|\n",
            "|  META|       381.68|\n",
            "|  TSLA|       411.47|\n",
            "|   QQQ|       405.57|\n",
            "|     V|       250.05|\n",
            "| GOOGL|       151.25|\n",
            "|   SPY|       479.22|\n",
            "|  AMZN|        187.2|\n",
            "|  MSFT|       344.62|\n",
            "|  NVDA|       405.95|\n",
            "+------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "cleaned_stocks.groupBy(\"Ticker\").max(\"Open\").withColumnRenamed(\"max(Open)\", \"MaxStockPrice\").show(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfIX4QOFMrdo",
        "outputId": "7b4f5b2f-8859-4147-ddb9-ab780b821a7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------------+\n",
            "|Ticker|MaxStockPrice|\n",
            "+------+-------------+\n",
            "| BRK-B|       361.39|\n",
            "|   TSM|       141.61|\n",
            "|  AAPL|       182.63|\n",
            "|  META|       381.68|\n",
            "|  TSLA|       411.47|\n",
            "|   QQQ|       405.57|\n",
            "|     V|       250.05|\n",
            "| GOOGL|       151.25|\n",
            "|   SPY|       479.22|\n",
            "|  AMZN|        187.2|\n",
            "|  MSFT|       344.62|\n",
            "|  NVDA|       405.95|\n",
            "+------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cleaned_stocks.groupBy(\"Ticker\").agg(func.max(\"Open\").alias(\"MaxStockPrice\")).show(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_qbiZe0Mrdo",
        "outputId": "a09d6c4f-0b41-4806-a6be-51ab7c55f67f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------------+------------+\n",
            "|Ticker|MaxStockPrice| TotalVolume|\n",
            "+------+-------------+------------+\n",
            "| BRK-B|       361.39|  5862401321|\n",
            "|   TSM|       141.61| 12506470104|\n",
            "|  AAPL|       182.63|139310061360|\n",
            "|  META|       381.68| 30148848043|\n",
            "|  TSLA|       411.47|171802975076|\n",
            "|   QQQ|       405.57| 60437153773|\n",
            "|     V|       250.05| 10410997871|\n",
            "| GOOGL|       151.25| 43956560981|\n",
            "|   SPY|       479.22|107925285300|\n",
            "|  AMZN|        187.2|104503287430|\n",
            "|  MSFT|       344.62| 37976660472|\n",
            "|  NVDA|       405.95| 58787218324|\n",
            "+------+-------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cleaned_stocks.groupBy(\"Ticker\").agg(\n",
        "    func.max(\"Open\").alias(\"MaxStockPrice\"),\n",
        "    func.sum(\"Volume\").alias(\"TotalVolume\")\n",
        ").show(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj3Y-Vz7Mrdp",
        "outputId": "2c96b11e-5d21-40e8-88e1-8ea6080c557c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+-------+------+------+------+------+----+-----+---+----+\n",
            "|Ticker|ParsedDate| Volume|  Open|   Low|  High| Close|Year|Month|Day|Week|\n",
            "+------+----------+-------+------+------+------+------+----+-----+---+----+\n",
            "| BRK-B|2023-05-31|6175417|321.12|319.39|322.41|321.08|2023|    5| 31|  22|\n",
            "| BRK-B|2023-05-30|3232461|321.86| 319.0|322.47|322.19|2023|    5| 30|  22|\n",
            "| BRK-B|2023-05-26|3229873|320.44|319.67|322.63| 320.6|2023|    5| 26|  21|\n",
            "| BRK-B|2023-05-25|4251935|320.56|317.71|320.56|319.02|2023|    5| 25|  21|\n",
            "| BRK-B|2023-05-24|3075393|322.71|319.56| 323.0| 320.2|2023|    5| 24|  21|\n",
            "| BRK-B|2023-05-23|4031342|328.19|322.97|329.27|323.11|2023|    5| 23|  21|\n",
            "| BRK-B|2023-05-22|2763422|330.75|328.35|331.49|329.13|2023|    5| 22|  21|\n",
            "| BRK-B|2023-05-19|4323538| 331.0|329.12|333.94|330.39|2023|    5| 19|  20|\n",
            "| BRK-B|2023-05-18|2808329|326.87|325.85|329.98|329.76|2023|    5| 18|  20|\n",
            "| BRK-B|2023-05-17|3047626|325.02|324.82|328.26|327.39|2023|    5| 17|  20|\n",
            "+------+----------+-------+------+------+------+------+----+-----+---+----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Calculate maximum price of stocks each year => Basic date manipulation operation\n",
        "cleaned_stocks = (cleaned_stocks.withColumn(\"Year\", func.year(cleaned_stocks.ParsedDate))\n",
        "                                .withColumn(\"Month\", func.month(cleaned_stocks.ParsedDate))\n",
        "                                .withColumn(\"Day\", func.dayofmonth(cleaned_stocks.ParsedDate))\n",
        "                                .withColumn(\"Week\", func.weekofyear(cleaned_stocks.ParsedDate))\n",
        "                 )\n",
        "cleaned_stocks.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx0WSS_dMrdp",
        "outputId": "f8d8f542-44f4-4a46-c2f9-0693d6ba12b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----+-----+---------+--------+\n",
            "|Ticker|Year|Month|MonthHigh|MonthLow|\n",
            "+------+----+-----+---------+--------+\n",
            "| BRK-B|2022|   10|   297.98|  260.58|\n",
            "|  META|2020|    6|   241.28|  209.75|\n",
            "| BRK-B|2018|    9|   222.13|  209.21|\n",
            "|  MSFT|2022|    6|    275.2|  243.86|\n",
            "|  MSFT|2021|    2|   245.03|  230.01|\n",
            "|     V|2020|    7|    200.0|   189.5|\n",
            "|   SPY|2019|    6|   296.04|  275.31|\n",
            "|   QQQ|2019|    6|   189.72|  171.98|\n",
            "|   QQQ|2019|    2|    174.2|  166.73|\n",
            "|  MSFT|2020|    1|   174.05|  157.08|\n",
            "| BRK-B|2021|   10|   290.85|  273.02|\n",
            "| BRK-B|2020|   10|   216.74|  200.03|\n",
            "|  TSLA|2023|    4|   199.91|  152.64|\n",
            "|  TSLA|2019|    4|    19.22|   15.72|\n",
            "|  AMZN|2018|    5|    81.15|   81.15|\n",
            "|     V|2020|    4|   181.78|  152.53|\n",
            "| GOOGL|2018|    5|     54.1|    54.1|\n",
            "|  NVDA|2021|    3|    139.0|   121.3|\n",
            "|  NVDA|2019|    6|    41.25|   33.98|\n",
            "|     V|2018|    7|   143.08|  131.96|\n",
            "+------+----+-----+---------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Calculate average stock price for stock each month\n",
        "monthly = cleaned_stocks.groupBy(['Ticker', 'Year', 'Month']).agg(func.max(\"Open\").alias(\"MonthHigh\"), func.min(\"Open\").alias(\"MonthLow\"))\n",
        "weekly = cleaned_stocks.groupBy(['Ticker', 'Year', 'Week']).agg(func.max(\"Open\").alias(\"WeekHigh\"), func.min(\"Open\").alias(\"WeekLow\"))\n",
        "monthly.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNYHOTeHMrdp",
        "outputId": "7960c529-50b5-4d61-8712-d15360d8a105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----+----+--------+-------+\n",
            "|Ticker|Year|Week|WeekHigh|WeekLow|\n",
            "+------+----+----+--------+-------+\n",
            "| BRK-B|2022|  14|   352.0| 341.17|\n",
            "| BRK-B|2022|  10|  326.59| 322.49|\n",
            "| BRK-B|2021|  14|  264.22| 260.02|\n",
            "|  META|2022|  43|  131.68|  97.98|\n",
            "|  META|2020|   6|  212.51| 203.44|\n",
            "|  TSLA|2022|  20|  255.72| 235.67|\n",
            "|  TSLA|2020|  19|   52.92|  46.73|\n",
            "|  TSLA|2020|  16|   51.49|  39.34|\n",
            "|  TSLA|2018|  39|   20.86|  18.02|\n",
            "| GOOGL|2021|  29|  130.43| 125.53|\n",
            "|  NVDA|2020|  37|  129.89| 117.35|\n",
            "|   SPY|2022|  42|  375.13| 364.01|\n",
            "|   SPY|2021|  52|  477.93| 472.06|\n",
            "|   QQQ|2022|  34|  320.28| 313.61|\n",
            "|   QQQ|2022|  29|  306.43| 293.11|\n",
            "|   QQQ|2021|  45|  399.16| 391.77|\n",
            "| BRK-B|2018|  48|  217.23|  209.3|\n",
            "|  MSFT|2022|   6|  309.87| 301.25|\n",
            "|  MSFT|2021|   2|  218.47| 213.52|\n",
            "|  META|2022|  40|  140.49| 136.76|\n",
            "+------+----+----+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "weekly.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIIi0JEmMrdp",
        "outputId": "519d2859-d8ca-44d1-d90d-c55c702f109f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----+----+--------+-------+---------+\n",
            "|Ticker|Year|Week|WeekHigh|WeekLow|   Spread|\n",
            "+------+----+----+--------+-------+---------+\n",
            "| BRK-B|2022|  14|   352.0| 341.17|10.829987|\n",
            "| BRK-B|2022|  10|  326.59| 322.49| 4.100006|\n",
            "| BRK-B|2021|  14|  264.22| 260.02| 4.200012|\n",
            "|  META|2022|  43|  131.68|  97.98| 33.69999|\n",
            "|  META|2020|   6|  212.51| 203.44| 9.069992|\n",
            "|  TSLA|2022|  20|  255.72| 235.67|20.050003|\n",
            "|  TSLA|2020|  19|   52.92|  46.73|6.1899986|\n",
            "|  TSLA|2020|  16|   51.49|  39.34|12.150002|\n",
            "|  TSLA|2018|  39|   20.86|  18.02|2.8400002|\n",
            "| GOOGL|2021|  29|  130.43| 125.53| 4.899994|\n",
            "|  NVDA|2020|  37|  129.89| 117.35|12.540001|\n",
            "|   SPY|2022|  42|  375.13| 364.01|11.119995|\n",
            "|   SPY|2021|  52|  477.93| 472.06| 5.869995|\n",
            "|   QQQ|2022|  34|  320.28| 313.61|6.6700134|\n",
            "|   QQQ|2022|  29|  306.43| 293.11|13.320007|\n",
            "|   QQQ|2021|  45|  399.16| 391.77|7.3900146|\n",
            "| BRK-B|2018|  48|  217.23|  209.3|7.9299927|\n",
            "|  MSFT|2022|   6|  309.87| 301.25| 8.619995|\n",
            "|  MSFT|2021|   2|  218.47| 213.52| 4.949997|\n",
            "|  META|2022|  40|  140.49| 136.76| 3.730011|\n",
            "+------+----+----+--------+-------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "weekly.withColumn(\"Spread\", weekly['WeekHigh'] - weekly['WeekLow']).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffKhv3iMMrdp",
        "outputId": "0559ab88-a7f3-4004-cbce-35dd049e56b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----+---------+---------+\n",
            "|Ticker|Year|YearlHigh|YearlyLow|\n",
            "+------+----+---------+---------+\n",
            "|  NVDA|2020|   147.04|    50.02|\n",
            "|     V|2022|   233.04|    175.0|\n",
            "|  META|2020|   300.16|   139.75|\n",
            "| BRK-B|2023|    331.0|   294.68|\n",
            "|   TSM|2018|     45.0|    35.33|\n",
            "|   TSM|2021|   141.61|    108.0|\n",
            "|   SPY|2022|   479.22|  349.205|\n",
            "|  MSFT|2019|   159.45|    99.55|\n",
            "|  MSFT|2021|   344.62|   212.17|\n",
            "| BRK-B|2018|    224.0|   185.43|\n",
            "|  META|2021|   381.68|    247.9|\n",
            "|  TSLA|2019|     29.0|    12.07|\n",
            "|   QQQ|2020|   314.16|   170.92|\n",
            "|  META|2018|   215.72|    123.1|\n",
            "|  AAPL|2020|   138.05|    57.02|\n",
            "|  MSFT|2020|   229.27|   137.01|\n",
            "|  TSLA|2021|   411.47|   184.18|\n",
            "|     V|2021|   250.05|    192.0|\n",
            "|  NVDA|2018|    72.33|    31.62|\n",
            "|   SPY|2020|   373.81|   228.19|\n",
            "+------+----+---------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "yearly = cleaned_stocks.groupBy(['Ticker', 'Year']).agg(func.max(\"Open\").alias(\"YearlHigh\"), func.min(\"Open\").alias(\"YearlyLow\"))\n",
        "yearly.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYPxckehMrdp",
        "outputId": "8e511bc2-1dd3-45df-e202-242f036b668d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+-------+------+------+------+------+----+-----+---+----+------+----+---------+---------+\n",
            "|Ticker|ParsedDate| Volume|  Open|   Low|  High| Close|Year|Month|Day|Week|Ticker|Year|YearlHigh|YearlyLow|\n",
            "+------+----------+-------+------+------+------+------+----+-----+---+----+------+----+---------+---------+\n",
            "| BRK-B|2023-05-31|6175417|321.12|319.39|322.41|321.08|2023|    5| 31|  22| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-30|3232461|321.86| 319.0|322.47|322.19|2023|    5| 30|  22| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-26|3229873|320.44|319.67|322.63| 320.6|2023|    5| 26|  21| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-25|4251935|320.56|317.71|320.56|319.02|2023|    5| 25|  21| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-24|3075393|322.71|319.56| 323.0| 320.2|2023|    5| 24|  21| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-23|4031342|328.19|322.97|329.27|323.11|2023|    5| 23|  21| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-22|2763422|330.75|328.35|331.49|329.13|2023|    5| 22|  21| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-19|4323538| 331.0|329.12|333.94|330.39|2023|    5| 19|  20| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-18|2808329|326.87|325.85|329.98|329.76|2023|    5| 18|  20| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-17|3047626|325.02|324.82|328.26|327.39|2023|    5| 17|  20| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-16|2139996|322.46|322.36|324.69|323.75|2023|    5| 16|  20| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-15|2191609|322.89|320.13|323.83|323.53|2023|    5| 15|  20| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-12|1938264|323.82|320.54|324.24|322.49|2023|    5| 12|  19| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-11|2549339| 321.0|319.81|322.96|322.64|2023|    5| 11|  19| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-10|2641134|326.08|320.15|326.16|322.99|2023|    5| 10|  19| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-09|2285924|324.87|323.48|326.88|324.87|2023|    5|  9|  19| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-08|3303393|328.26|325.79|330.69|326.14|2023|    5|  8|  19| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-05|3876299|323.36|322.62|325.16|323.88|2023|    5|  5|  18| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-04|3194768|323.44|317.41|325.99| 320.0|2023|    5|  4|  18| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-03|2660456|327.13|323.06|328.07|323.22|2023|    5|  3|  18| BRK-B|2023|    331.0|   294.68|\n",
            "+------+----------+-------+------+------+------+------+----+-----+---+----+------+----+---------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Joins\n",
        "cleaned_stocks.join(yearly, (cleaned_stocks.Ticker==yearly.Ticker) & (cleaned_stocks.Year == yearly.Year),\n",
        "                                      'inner'\n",
        "                                     ).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hD56ZX8Mrdq",
        "outputId": "36e944b9-756c-4db4-c463-5aa65ab45543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------+------+------+------+------+-----+---+----+------+----+---------+---------+\n",
            "|ParsedDate| Volume|  Open|   Low|  High| Close|Month|Day|Week|Ticker|Year|YearlHigh|YearlyLow|\n",
            "+----------+-------+------+------+------+------+-----+---+----+------+----+---------+---------+\n",
            "|2023-05-31|6175417|321.12|319.39|322.41|321.08|    5| 31|  22| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-30|3232461|321.86| 319.0|322.47|322.19|    5| 30|  22| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-26|3229873|320.44|319.67|322.63| 320.6|    5| 26|  21| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-25|4251935|320.56|317.71|320.56|319.02|    5| 25|  21| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-24|3075393|322.71|319.56| 323.0| 320.2|    5| 24|  21| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-23|4031342|328.19|322.97|329.27|323.11|    5| 23|  21| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-22|2763422|330.75|328.35|331.49|329.13|    5| 22|  21| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-19|4323538| 331.0|329.12|333.94|330.39|    5| 19|  20| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-18|2808329|326.87|325.85|329.98|329.76|    5| 18|  20| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-17|3047626|325.02|324.82|328.26|327.39|    5| 17|  20| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-16|2139996|322.46|322.36|324.69|323.75|    5| 16|  20| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-15|2191609|322.89|320.13|323.83|323.53|    5| 15|  20| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-12|1938264|323.82|320.54|324.24|322.49|    5| 12|  19| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-11|2549339| 321.0|319.81|322.96|322.64|    5| 11|  19| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-10|2641134|326.08|320.15|326.16|322.99|    5| 10|  19| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-09|2285924|324.87|323.48|326.88|324.87|    5|  9|  19| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-08|3303393|328.26|325.79|330.69|326.14|    5|  8|  19| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-05|3876299|323.36|322.62|325.16|323.88|    5|  5|  18| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-04|3194768|323.44|317.41|325.99| 320.0|    5|  4|  18| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-03|2660456|327.13|323.06|328.07|323.22|    5|  3|  18| BRK-B|2023|    331.0|   294.68|\n",
            "+----------+-------+------+------+------+------+-----+---+----+------+----+---------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cleaned_stocks.join(yearly,\n",
        "                    (cleaned_stocks.Ticker==yearly.Ticker) & (cleaned_stocks.Year == yearly.Year),\n",
        "                    'inner'\n",
        "                   ).drop(yearly.Year, yearly.Ticker).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3CxBbjcMrdq"
      },
      "outputs": [],
      "source": [
        "historic_stocks = cleaned_stocks.join(yearly,\n",
        "                    (cleaned_stocks.Ticker==yearly.Ticker) & (cleaned_stocks.Year == yearly.Year),\n",
        "                    'inner'\n",
        "                   ).drop(yearly.Year, yearly.Ticker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEiTVxIoMrdq",
        "outputId": "e3d417a6-9c6c-47b8-d980-814a9be3efff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------+------+------+------+------+-----+---+----+------+----+---------+---------+\n",
            "|ParsedDate| Volume|  Open|   Low|  High| Close|Month|Day|Week|Ticker|Year|YearlHigh|YearlyLow|\n",
            "+----------+-------+------+------+------+------+-----+---+----+------+----+---------+---------+\n",
            "|2023-05-31|6175417|321.12|319.39|322.41|321.08|    5| 31|  22| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-30|3232461|321.86| 319.0|322.47|322.19|    5| 30|  22| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-26|3229873|320.44|319.67|322.63| 320.6|    5| 26|  21| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-25|4251935|320.56|317.71|320.56|319.02|    5| 25|  21| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-24|3075393|322.71|319.56| 323.0| 320.2|    5| 24|  21| BRK-B|2023|    331.0|   294.68|\n",
            "+----------+-------+------+------+------+------+-----+---+----+------+----+---------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "historic_stocks.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0DMkNaZMrdq",
        "outputId": "5b77ac39-783a-4e04-d81b-b63fa048844f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+------+\n",
            "|Ticker|ParsedDate|  Open|\n",
            "+------+----------+------+\n",
            "| BRK-B|2023-05-31|321.12|\n",
            "| BRK-B|2023-05-30|321.86|\n",
            "| BRK-B|2023-05-26|320.44|\n",
            "| BRK-B|2023-05-25|320.56|\n",
            "| BRK-B|2023-05-24|322.71|\n",
            "| BRK-B|2023-05-23|328.19|\n",
            "| BRK-B|2023-05-22|330.75|\n",
            "| BRK-B|2023-05-19| 331.0|\n",
            "| BRK-B|2023-05-18|326.87|\n",
            "| BRK-B|2023-05-17|325.02|\n",
            "| BRK-B|2023-05-16|322.46|\n",
            "| BRK-B|2023-05-15|322.89|\n",
            "| BRK-B|2023-05-12|323.82|\n",
            "| BRK-B|2023-05-11| 321.0|\n",
            "| BRK-B|2023-05-10|326.08|\n",
            "| BRK-B|2023-05-09|324.87|\n",
            "| BRK-B|2023-05-08|328.26|\n",
            "| BRK-B|2023-05-05|323.36|\n",
            "| BRK-B|2023-05-04|323.44|\n",
            "| BRK-B|2023-05-03|327.13|\n",
            "+------+----------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "snapshot = cleaned_stocks.select(['Ticker', 'ParsedDate', 'Open'])\n",
        "snapshot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YlP5kjiMrdq",
        "outputId": "270f5ab8-5bbf-4751-8de9-76d3aa77818f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+-----+------------+\n",
            "|Ticker|ParsedDate| Open|PreviousOpen|\n",
            "+------+----------+-----+------------+\n",
            "|  AAPL|2018-05-31|46.81|        NULL|\n",
            "|  AAPL|2018-06-01| 47.0|       46.81|\n",
            "|  AAPL|2018-06-04|47.91|        47.0|\n",
            "|  AAPL|2018-06-05|48.27|       47.91|\n",
            "|  AAPL|2018-06-06|48.41|       48.27|\n",
            "|  AAPL|2018-06-07|48.54|       48.41|\n",
            "|  AAPL|2018-06-08|47.79|       48.54|\n",
            "|  AAPL|2018-06-11|47.84|       47.79|\n",
            "|  AAPL|2018-06-12|47.85|       47.84|\n",
            "|  AAPL|2018-06-13|48.11|       47.85|\n",
            "|  AAPL|2018-06-14|47.89|       48.11|\n",
            "|  AAPL|2018-06-15|47.51|       47.89|\n",
            "|  AAPL|2018-06-18|46.97|       47.51|\n",
            "|  AAPL|2018-06-19|46.29|       46.97|\n",
            "|  AAPL|2018-06-20|46.59|       46.29|\n",
            "|  AAPL|2018-06-21|46.81|       46.59|\n",
            "|  AAPL|2018-06-22|46.53|       46.81|\n",
            "|  AAPL|2018-06-25|45.85|       46.53|\n",
            "|  AAPL|2018-06-26|45.75|       45.85|\n",
            "|  AAPL|2018-06-27|46.31|       45.75|\n",
            "+------+----------+-----+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lag1Day = Window.partitionBy(\"Ticker\").orderBy(\"ParsedDate\")\n",
        "snapshot.withColumn(\"PreviousOpen\", func.lag(\"Open\", 1).over(lag1Day)).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovIHGIcxMrdq",
        "outputId": "86f6e79c-0381-4342-f608-9af19269ed63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+-----+-----+\n",
            "|Ticker|ParsedDate| Open| MA50|\n",
            "+------+----------+-----+-----+\n",
            "|  AAPL|2018-05-31|46.81|46.81|\n",
            "|  AAPL|2018-06-01| 47.0|46.91|\n",
            "|  AAPL|2018-06-04|47.91|47.24|\n",
            "|  AAPL|2018-06-05|48.27| 47.5|\n",
            "|  AAPL|2018-06-06|48.41|47.68|\n",
            "|  AAPL|2018-06-07|48.54|47.82|\n",
            "|  AAPL|2018-06-08|47.79|47.82|\n",
            "|  AAPL|2018-06-11|47.84|47.82|\n",
            "|  AAPL|2018-06-12|47.85|47.82|\n",
            "|  AAPL|2018-06-13|48.11|47.85|\n",
            "|  AAPL|2018-06-14|47.89|47.86|\n",
            "|  AAPL|2018-06-15|47.51|47.83|\n",
            "|  AAPL|2018-06-18|46.97|47.76|\n",
            "|  AAPL|2018-06-19|46.29|47.66|\n",
            "|  AAPL|2018-06-20|46.59|47.59|\n",
            "|  AAPL|2018-06-21|46.81|47.54|\n",
            "|  AAPL|2018-06-22|46.53|47.48|\n",
            "|  AAPL|2018-06-25|45.85|47.39|\n",
            "|  AAPL|2018-06-26|45.75| 47.3|\n",
            "|  AAPL|2018-06-27|46.31|47.25|\n",
            "+------+----------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "## Calculate moving average\n",
        "movingAverage = Window.partitionBy(\"Ticker\").orderBy(\"ParsedDate\").rowsBetween(-50, 0)\n",
        "(snapshot.withColumn(\"MA50\", func.avg(\"Open\").over(movingAverage))\n",
        "         .withColumn(\"MA50\", func.round(\"MA50\", 2))).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n1SD3ZBMrdr",
        "outputId": "351b56b6-f92e-48c2-ddb2-62fe3579af7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+------+-------+\n",
            "|Ticker|ParsedDate|  Open|MaxOpen|\n",
            "+------+----------+------+-------+\n",
            "|  AAPL|2022-01-04|182.63|      1|\n",
            "|  AAPL|2021-12-13|181.12|      2|\n",
            "|  AAPL|2021-12-28|180.16|      3|\n",
            "|  AAPL|2022-01-05|179.61|      4|\n",
            "|  AAPL|2021-12-30|179.47|      5|\n",
            "|  AAPL|2021-12-29|179.33|      6|\n",
            "|  AAPL|2021-12-16|179.28|      7|\n",
            "|  AAPL|2022-03-30|178.55|      8|\n",
            "|  AAPL|2021-12-31|178.09|      9|\n",
            "|  AAPL|2022-03-31|177.84|     10|\n",
            "|  AAPL|2022-01-03|177.83|     11|\n",
            "|  AAPL|2022-04-05| 177.5|     12|\n",
            "|  AAPL|2023-05-31|177.33|     13|\n",
            "|  AAPL|2021-12-27|177.09|     14|\n",
            "|  AAPL|2023-05-30|176.96|     15|\n",
            "|  AAPL|2022-03-29|176.69|     16|\n",
            "|  AAPL|2023-05-19|176.39|     17|\n",
            "|  AAPL|2022-01-12|176.12|     18|\n",
            "|  AAPL|2022-02-09|176.05|     19|\n",
            "|  AAPL|2021-12-23|175.85|     20|\n",
            "+------+----------+------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "maximumStock = Window.partitionBy(\"Ticker\").orderBy(snapshot.Open.desc())\n",
        "snapshot.withColumn(\"MaxOpen\", func.row_number().over(maximumStock)).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOep-5UYMrdr",
        "outputId": "211032d3-0dc5-422d-9a83-3f0f8941fea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+------+-------+\n",
            "|Ticker|ParsedDate|  Open|MaxOpen|\n",
            "+------+----------+------+-------+\n",
            "|  AAPL|2022-01-04|182.63|      1|\n",
            "|  AAPL|2021-12-13|181.12|      2|\n",
            "|  AAPL|2021-12-28|180.16|      3|\n",
            "|  AAPL|2022-01-05|179.61|      4|\n",
            "|  AAPL|2021-12-30|179.47|      5|\n",
            "|  AMZN|2021-07-12| 187.2|      1|\n",
            "|  AMZN|2021-07-09|186.13|      2|\n",
            "|  AMZN|2021-07-07|185.87|      3|\n",
            "|  AMZN|2021-11-19|185.63|      4|\n",
            "|  AMZN|2021-07-14|185.44|      5|\n",
            "| BRK-B|2022-03-29|361.39|      1|\n",
            "| BRK-B|2022-03-28|360.59|      2|\n",
            "| BRK-B|2022-03-31| 359.0|      3|\n",
            "| BRK-B|2022-03-30|354.66|      4|\n",
            "| BRK-B|2022-03-25| 353.9|      5|\n",
            "| GOOGL|2022-02-02|151.25|      1|\n",
            "| GOOGL|2021-11-19|149.98|      2|\n",
            "| GOOGL|2021-11-08|149.83|      3|\n",
            "| GOOGL|2021-11-22|149.33|      4|\n",
            "| GOOGL|2021-11-09|149.23|      5|\n",
            "+------+----------+------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Calculate top 5 highest close price for each stock in a year\n",
        "snapshot.withColumn(\"MaxOpen\", func.row_number().over(maximumStock)).filter(\"MaxOpen<=5\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ERtcqOlMrdr",
        "outputId": "3f4fc7b0-bec4-4f00-9c53-5e24d4ace63a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+-----+-----+\n",
            "|Ticker|ParsedDate| Open| MA50|\n",
            "+------+----------+-----+-----+\n",
            "|  AAPL|2018-05-31|46.81|46.81|\n",
            "|  AAPL|2018-06-01| 47.0|46.91|\n",
            "|  AAPL|2018-06-04|47.91|47.24|\n",
            "|  AAPL|2018-06-05|48.27| 47.5|\n",
            "|  AAPL|2018-06-06|48.41|47.68|\n",
            "|  AAPL|2018-06-07|48.54|47.82|\n",
            "|  AAPL|2018-06-08|47.79|47.82|\n",
            "|  AAPL|2018-06-11|47.84|47.82|\n",
            "|  AAPL|2018-06-12|47.85|47.82|\n",
            "|  AAPL|2018-06-13|48.11|47.85|\n",
            "|  AAPL|2018-06-14|47.89|47.86|\n",
            "|  AAPL|2018-06-15|47.51|47.83|\n",
            "|  AAPL|2018-06-18|46.97|47.76|\n",
            "|  AAPL|2018-06-19|46.29|47.66|\n",
            "|  AAPL|2018-06-20|46.59|47.59|\n",
            "|  AAPL|2018-06-21|46.81|47.54|\n",
            "|  AAPL|2018-06-22|46.53|47.48|\n",
            "|  AAPL|2018-06-25|45.85|47.39|\n",
            "|  AAPL|2018-06-26|45.75| 47.3|\n",
            "|  AAPL|2018-06-27|46.31|47.25|\n",
            "+------+----------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result = snapshot.withColumn(\"MaxOpen\", func.row_number().over(maximumStock)).filter(\"MaxOpen<=5\")\n",
        "moving_avg = (snapshot.withColumn(\"MA50\", func.avg(\"Open\").over(movingAverage))\n",
        "         .withColumn(\"MA50\", func.round(\"MA50\", 2)))\n",
        "moving_avg.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1vFlU2eMrdr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # #from pyspark import SparkConf,SparkContext\n",
        "# # from pyspark.sql import SparkSession\n",
        "# # import collections\n",
        "# # spark = SparkSession.builder.master(\"local\").appName(\"Ingestion\").config('spark.ui.port', '4050').getOrCreate()\n",
        "\n",
        "# df_csv = spark.read.format('csv') \\\n",
        "#                 .option(\"inferSchema\",\"true\") \\\n",
        "#                 .option(\"header\",\"true\") \\\n",
        "#                 .option(\"sep\",\";\") \\\n",
        "#                 .load(\"/content/drive/MyDrive/data/DataFormat/people.csv\")\n",
        "\n",
        "# df_json = spark.read.format('json') \\\n",
        "#                 .option(\"inferSchema\",\"true\") \\\n",
        "#                 .option(\"header\",\"true\") \\\n",
        "#                 .option(\"sep\",\";\") \\\n",
        "#                 .load(\"/content/drive/MyDrive/data/DataFormat/people.json\")\n",
        "\n",
        "# peopleDF = spark.read.json(\"/content/drive/MyDrive/data/DataFormat/people.json\")\n",
        "\n",
        "# import pyarrow.csv as pv\n",
        "# import pyarrow.parquet as pq\n",
        "\n",
        "# # read hdb resale price\n",
        "# hdb_table = pv.read_csv(\"/content/drive/MyDrive/data/DataFormat/resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.csv\")\n",
        "# # convert the CSV file to a Parquet file\n",
        "# pq.write_table(hdb_table,'resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.parquet')\n",
        "# hdb_parquet = pq.ParquetFile('resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.parquet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY-xZTgUMrdr"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlkA5J6DMrds",
        "outputId": "17fed482-0b12-4e22-a9b8-b271b7062804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+---+---------+\n",
            "| name|age|      job|\n",
            "+-----+---+---------+\n",
            "|Jorge| 30|Developer|\n",
            "|  Bob| 32|Developer|\n",
            "+-----+---+---------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read CSV file people.csv\n",
        "# Show result\n",
        "df_csv.show()\n",
        "# Print schema\n",
        "df_csv.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgofe1BoMrds"
      },
      "outputs": [],
      "source": [
        "# # Write csv file\n",
        "# df = spark.read.format('csv') \\\n",
        "#                 .option(\"inferSchema\",\"true\") \\\n",
        "#                 .option(\"header\",\"true\") \\\n",
        "#                 .option(\"sep\",\";\") \\\n",
        "#                 .load(\"people.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pnm-h-KYMrds"
      },
      "outputs": [],
      "source": [
        "# DataFrames can be saved as Parquet files, maintaining the schema information.\n",
        "peopleDF.write.format(\"parquet\").mode(\"overwrite\").save(\"people.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPfR4MzpMrds",
        "outputId": "b19ebc3c-324d-4b74-8af8-7dd19463a9e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+\n",
            "|  name|\n",
            "+------+\n",
            "|Justin|\n",
            "+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read in the Parquet file created above.\n",
        "# Parquet files are self-describing so the schema is preserved.\n",
        "# The result of loading a parquet file is also a DataFrame.\n",
        "parquetFile = spark.read.parquet(\"people.parquet\")\n",
        "\n",
        "# Parquet files can also be used to create a temporary view and then used in SQL statements.\n",
        "parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
        "teenagers = spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\")\n",
        "teenagers.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OES5zPdTMrds",
        "outputId": "ee122760-49d0-4393-b622-cd9404ce14d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pyarrow._parquet.FileMetaData object at 0x7d4befa652b0>\n",
            "  created_by: parquet-cpp-arrow version 10.0.1\n",
            "  num_columns: 10\n",
            "  num_rows: 52203\n",
            "  num_row_groups: 1\n",
            "  format_version: 2.6\n",
            "  serialized_size: 2079\n",
            "<pyarrow._parquet.RowGroupMetaData object at 0x7d4be41243b0>\n",
            "  num_columns: 10\n",
            "  num_rows: 52203\n",
            "  total_byte_size: 431095\n",
            "<pyarrow._parquet.Statistics object at 0x7d4bdebd4c70>\n",
            "  has_min_max: True\n",
            "  min: 195000.0\n",
            "  max: 1088888.0\n",
            "  null_count: 0\n",
            "  distinct_count: 0\n",
            "  num_values: 52203\n",
            "  physical_type: DOUBLE\n",
            "  logical_type: None\n",
            "  converted_type (legacy): NONE\n"
          ]
        }
      ],
      "source": [
        "# inspect the parquet metadata\n",
        "print(hdb_parquet.metadata)\n",
        "# inspect the parquet row group metadata\n",
        "print(hdb_parquet.metadata.row_group(0))\n",
        "# inspect the column chunk metadata\n",
        "print(hdb_parquet.metadata.row_group(0).column(9).statistics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming the target variable is \"class\" and other columns are features\n",
        "feature_cols = iris_data.columns[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert string labels into numerical labels\n",
        "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
        "iris_data = indexer.fit(iris_data).transform(iris_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a feature vector by assembling the feature columns\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "data = assembler.transform(iris_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "(training_data, testing_data) = data.randomSplit([0.8, 0.2], seed=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Customized parameters\n",
        "num_trees = 10\n",
        "max_depth = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train a RandomForestClassifier with customized parameters\n",
        "rf = RandomForestClassifier(\n",
        "    labelCol=\"label\",\n",
        "    featuresCol=\"features\",\n",
        "    numTrees=num_trees,\n",
        "    maxDepth=max_depth\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = rf.fit(training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on the testing data\n",
        "predictions = model.transform(testing_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.97\n"
          ]
        }
      ],
      "source": [
        "# Print the accuracy\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Importances:  (4,[0,1,2,3],[0.10971043638291314,0.027259903033869593,0.5388647415001908,0.3241649190830265])\n"
          ]
        }
      ],
      "source": [
        "# Show the feature importances\n",
        "print(\"Feature Importances: \", model.featureImportances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop the Spark session\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
