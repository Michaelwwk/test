{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suriarasai/BEAD2024/blob/main/colab/03_Apache_Spark_Core_RDD_Operations_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaVSpe-PbxkO"
      },
      "source": [
        "# Introduction to Apache PySpark\n",
        "In this demo we will see how we can run PySpark in a Google\n",
        " Colaboratory notebook. We will also perform some basic data exploratory tasks common to data science problems.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26ugKWTTdmm-"
      },
      "source": [
        "## PySpark Install\n",
        "\n",
        "The first step involves installing pyspark.  The next step is to install findspark library.\n",
        "\n",
        "*Note: the --ignore-install flag is used to ignore previous installations and use the latest one built alongside the allocated cluster.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cdc0oJjb8N5",
        "outputId": "b275d02a-6720-42db-8718-b5421d89857b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# install pyspark using pip\n",
        "!pip install --ignore-install -q pyspark\n",
        "# install findspark using pip\n",
        "!pip install --ignore-install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pyarrow.csv as pv\n",
        "import pyarrow.parquet as pq\n",
        "import collections\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from google.colab import drive\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DateType\n",
        "from datetime import datetime\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.types import IntegerType\n",
        "import pyspark.sql.functions as func\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Stock Ticker\").config('spark.ui.port', '4050').getOrCreate()\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "someFile = \"wordcount.txt\"\n",
        "stock_folder = \"/content/drive/MyDrive/data/StockData\"\n",
        "stocks = spark.read.csv(stock_folder, header=True)\n",
        "peopleDF = spark.read.json(\"/content/drive/MyDrive/data/DataFormat/people.json\")\n",
        "\n",
        "df_csv = spark.read.format('csv') \\\n",
        "                .option(\"inferSchema\",\"true\") \\\n",
        "                .option(\"header\",\"true\") \\\n",
        "                .option(\"sep\",\";\") \\\n",
        "                .load(\"/content/drive/MyDrive/data/DataFormat/people.csv\")\n",
        "\n",
        "df_json = spark.read.format('json') \\\n",
        "                .option(\"inferSchema\",\"true\") \\\n",
        "                .option(\"header\",\"true\") \\\n",
        "                .option(\"sep\",\";\") \\\n",
        "                .load(\"/content/drive/MyDrive/data/DataFormat/people.json\")\n",
        "\n",
        "# read and convert hdb resale price\n",
        "hdb_table = pv.read_csv(\"/content/drive/MyDrive/data/DataFormat/resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.csv\")\n",
        "pq.write_table(hdb_table,'resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.parquet')\n",
        "hdb_parquet = pq.ParquetFile('resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7coAqaRcsJE"
      },
      "source": [
        "## Spark Session\n",
        "\n",
        "We import the basic object SparkSession from the Spark Framework. In PySpark, a Spark Session is a unified entry point for reading data, configuring the system, and managing various Spark services.\n",
        "\n",
        "Here's a breakdown of what the Spark Session does:\n",
        "\n",
        "1. Unified Entry Point: It's the central point to access all Spark  functionalities, making it simpler and more intuitive to use Spark for development.\n",
        "2. Data Reading and Writing: We use the Spark Session to read data from various sources (like HDFS, S3, JDBC, Hive, etc.) and write data to various sinks.\n",
        "3. Configuration Management: It allows us to configure various aspects of the Spark application, such as setting configuration parameters.\n",
        "4. Creating DataFrames and Datasets: The Spark Session provides methods to create DataFrames and Datasets, which are the core data structures in Spark.\n",
        "5. Execution of SQL Queries: We can run SQL queries by using the Spark Session, especially when dealing with structured data.\n",
        "6. Managing Spark Services: It also helps in managing underlying Spark services like SparkContext, and it's the main point of interaction when dealing with structured data.\n",
        "\n",
        "In PySpark, a Spark Session is created using the SparkSession.builder method. Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1I1VynS4JBT"
      },
      "outputs": [],
      "source": [
        "# import collections\n",
        "# spark = SparkSession.builder.master(\"local\").appName(\"My App \").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "c-XvTqWhANe5",
        "outputId": "0bf5704d-31ad-4f0d-942f-3dbdaf59fbde"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://741fe1287677:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>First Spark and Colab Demo </code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local appName=First Spark and Colab Demo >"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux4bAHTnv70Z"
      },
      "source": [
        "## Line Count\n",
        "To count the number of lines from a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRm36sJXv7mr",
        "outputId": "427b8308-0424-4115-b469-e12b0871c146"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n"
          ]
        }
      ],
      "source": [
        "# the above file is under your pythonProject folder\n",
        "spark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n",
        "print(spark.read.text(someFile).count())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVsBhvHTvokF"
      },
      "source": [
        "## Mounting Google Drive\n",
        "Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddxRp3RNh1Fx",
        "outputId": "ea86fac6-89cb-4a76-c6ab-c068c9997239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# to read in data from a text file, first upload the data file into your google drive and then mount your google drive onto colab\n",
        "# to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True)\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL3V5wUNridx"
      },
      "source": [
        "## Actions and Transformation\n",
        "\n",
        "In PySpark, operations on RDDs can be broadly classified into two categories: transformations and actions. Transformations create a new RDD from an existing one, while actions return a value after running a computation on the RDD. Below are simple examples demonstrating the use of transformations and actions.\n",
        "\n",
        "###Transformations\n",
        "\n",
        "####Map\n",
        "Applies a function to each element and returns a new RDD.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3o9yPBOr-ss",
        "outputId": "da64485a-0238-47ba-d96f-2586e88e8eb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 4, 9, 16, 25)\n"
          ]
        }
      ],
      "source": [
        "rdd = spark.sparkContext.parallelize((1, 2, 3, 4, 5))\n",
        "# Traditional Python map(function, collection) (few MBs - GB fails)\n",
        "# Scalabale map (Peta - support)\n",
        "# iterablecollection.map(function) -> Object\n",
        "# collect() Object to collection\n",
        "print(tuple(rdd.map(lambda x: x * x).collect()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytxb4hDSsMkZ"
      },
      "source": [
        "####Filter\n",
        "Returns a new RDD containing only the elements that satisfy a condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJKFicO-sSrR",
        "outputId": "ef73dd89-bfb6-4f44-8ea9-e68acf16d0c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 4]\n"
          ]
        }
      ],
      "source": [
        "print(rdd.filter(lambda x: x % 2 == 0).collect())  # Keeps even numbers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hK0FzEUsZQf"
      },
      "source": [
        "####FlatMap\n",
        "Similar to map, but each input item can be mapped to 0 or more output items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "0BrAp7Uwskim",
        "outputId": "5d15e8bf-e86b-4917-f3ee-7ea473fbb3fc"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'spark' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9ba7737d0a7a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hello world\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hello mars\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hello jupiter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hello saturn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
          ]
        }
      ],
      "source": [
        "words = spark.sparkContext.parallelize([\"hello world\", \"hi\", \"hello mars\", \"hello jupiter\", \"hello saturn\"])\n",
        "print(words.flatMap(lambda x: x.split(\" \")).collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDWTyCeks1iE"
      },
      "source": [
        "####Distinct\n",
        "Returns a new RDD containing distinct elements from the original RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74xIHlGchlEE"
      },
      "outputs": [],
      "source": [
        "print(words.flatMap(lambda x: x.split(\" \")).distinct().collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGk2e8kLs6cr",
        "outputId": "4247aa81-7e76-4fd3-c62b-21ef740bda9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4]\n"
          ]
        }
      ],
      "source": [
        "tuples = spark.sparkContext.parallelize((1, 1, 2, 3, 3, 4))\n",
        "print(tuples.distinct().collect())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO6zm2ZgtPYr"
      },
      "source": [
        "###Actions\n",
        "####Collect\n",
        "Returns all the elements of the RDD as an array to the driver program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrJG5bFCtaIy",
        "outputId": "007249d5-f22e-479f-d903-28bea457f2e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4]\n"
          ]
        }
      ],
      "source": [
        "print(tuples.distinct().collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9B3hqYsth7z"
      },
      "source": [
        "####Count\n",
        "Returns the number of elements in the RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yi1FyZRtlcn",
        "outputId": "615a3bc1-7110-4af2-e2ba-253499c589d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "print(tuples.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6AhdGsltqht"
      },
      "source": [
        "####Take\n",
        "Returns an array with the first n elements of the RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_EERBI9tuQu",
        "outputId": "275354cd-1635-49d3-9003-ea246ec4b963"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1, 2]\n"
          ]
        }
      ],
      "source": [
        "first_three = tuples.take(3)\n",
        "print(first_three)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q56QYaL7N4OH",
        "outputId": "653b986c-d3c4-43b7-bb9b-a83051d9de38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(Index=1, Customer Id='DD37Cf93aecA6Dc', First Name='Sheryl', Last Name='Baxter', Company='Rasmussen Group', City='East Leonard', Country='Chile', Phone 1='229.077.5154', Phone 2='397.884.0519x718', Email='zunigavanessa@smith.info', Subscription Date=datetime.date(2020, 8, 24), Website='http://www.stephenson.com/'),\n",
              " Row(Index=2, Customer Id='1Ef7b82A4CAAD10', First Name='Preston', Last Name='Lozano', Company='Vega-Gentry', City='East Jimmychester', Country='Djibouti', Phone 1='5153435776', Phone 2='686-620-1820x944', Email='vmata@colon.com', Subscription Date=datetime.date(2021, 4, 23), Website='http://www.hobbs.com/')]"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "customers.take(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk6Xizictyfh"
      },
      "source": [
        "####Reduce\n",
        "Aggregates the elements of the RDD using a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY7SkGfvt2-g",
        "outputId": "6ce076a7-ed02-4afd-cb3c-f57a1b9a40c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14\n"
          ]
        }
      ],
      "source": [
        "sum = tuples.reduce(lambda a, b: a + b)\n",
        "print(sum)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX1Ur6gyqFDk"
      },
      "source": [
        "These examples illustrate basic operations in PySpark, allowing you to manipulate and analyze large datasets efficiently. To run these examples, ensure you have a SparkContext (sc) initialized in your PySpark environment.\n",
        "\n",
        "### How to pretty print in PySaprk?\n",
        "\n",
        "The take() function and iteration in PySpark will mimic the pretty print function, but use them wisely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y1ezBHTOrAf",
        "outputId": "a2ab1811-5840-41a6-ce19-67fd7ccf9da7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretty printing has been turned OFF\n"
          ]
        }
      ],
      "source": [
        "pprint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QcNCFRZqkiL",
        "outputId": "fbfda114-1368-4df3-af84-9bd9725b0f3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First five records of customer data set [Row(Index=1, Customer Id='DD37Cf93aecA6Dc', First Name='Sheryl', Last Name='Baxter', Company='Rasmussen Group', City='East Leonard', Country='Chile', Phone 1='229.077.5154', Phone 2='397.884.0519x718', Email='zunigavanessa@smith.info', Subscription Date=datetime.date(2020, 8, 24), Website='http://www.stephenson.com/'), Row(Index=2, Customer Id='1Ef7b82A4CAAD10', First Name='Preston', Last Name='Lozano', Company='Vega-Gentry', City='East Jimmychester', Country='Djibouti', Phone 1='5153435776', Phone 2='686-620-1820x944', Email='vmata@colon.com', Subscription Date=datetime.date(2021, 4, 23), Website='http://www.hobbs.com/'), Row(Index=3, Customer Id='6F94879bDAfE5a6', First Name='Roy', Last Name='Berry', Company='Murillo-Perry', City='Isabelborough', Country='Antigua and Barbuda', Phone 1='+1-539-402-0259', Phone 2='(496)978-3969x58947', Email='beckycarr@hogan.com', Subscription Date=datetime.date(2020, 3, 25), Website='http://www.lawrence.com/'), Row(Index=4, Customer Id='5Cef8BFA16c5e3c', First Name='Linda', Last Name='Olsen', Company='Dominguez, Mcmillan and Donovan', City='Bensonview', Country='Dominican Republic', Phone 1='001-808-617-6467x12895', Phone 2='+1-813-324-8756', Email='stanleyblackwell@benson.org', Subscription Date=datetime.date(2020, 6, 2), Website='http://www.good-lyons.com/'), Row(Index=5, Customer Id='053d585Ab6b3159', First Name='Joanna', Last Name='Bender', Company='Martin, Lang and Andrade', City='West Priscilla', Country='Slovakia (Slovak Republic)', Phone 1='001-234-203-0635x76146', Phone 2='001-199-446-3860x3486', Email='colinalvarado@miles.net', Subscription Date=datetime.date(2021, 4, 17), Website='https://goodwin-ingram.com/')]\n",
            "Not so pretty....\n",
            "Now let us pretty print:\n",
            "Row(Index=1, Customer Id='DD37Cf93aecA6Dc', First Name='Sheryl', Last Name='Baxter', Company='Rasmussen Group', City='East Leonard', Country='Chile', Phone 1='229.077.5154', Phone 2='397.884.0519x718', Email='zunigavanessa@smith.info', Subscription Date=datetime.date(2020, 8, 24), Website='http://www.stephenson.com/')\n",
            "Row(Index=2, Customer Id='1Ef7b82A4CAAD10', First Name='Preston', Last Name='Lozano', Company='Vega-Gentry', City='East Jimmychester', Country='Djibouti', Phone 1='5153435776', Phone 2='686-620-1820x944', Email='vmata@colon.com', Subscription Date=datetime.date(2021, 4, 23), Website='http://www.hobbs.com/')\n",
            "Row(Index=3, Customer Id='6F94879bDAfE5a6', First Name='Roy', Last Name='Berry', Company='Murillo-Perry', City='Isabelborough', Country='Antigua and Barbuda', Phone 1='+1-539-402-0259', Phone 2='(496)978-3969x58947', Email='beckycarr@hogan.com', Subscription Date=datetime.date(2020, 3, 25), Website='http://www.lawrence.com/')\n",
            "Row(Index=4, Customer Id='5Cef8BFA16c5e3c', First Name='Linda', Last Name='Olsen', Company='Dominguez, Mcmillan and Donovan', City='Bensonview', Country='Dominican Republic', Phone 1='001-808-617-6467x12895', Phone 2='+1-813-324-8756', Email='stanleyblackwell@benson.org', Subscription Date=datetime.date(2020, 6, 2), Website='http://www.good-lyons.com/')\n",
            "Row(Index=5, Customer Id='053d585Ab6b3159', First Name='Joanna', Last Name='Bender', Company='Martin, Lang and Andrade', City='West Priscilla', Country='Slovakia (Slovak Republic)', Phone 1='001-234-203-0635x76146', Phone 2='001-199-446-3860x3486', Email='colinalvarado@miles.net', Subscription Date=datetime.date(2021, 4, 17), Website='https://goodwin-ingram.com/')\n",
            "Row(Index=6, Customer Id='2d08FB17EE273F4', First Name='Aimee', Last Name='Downs', Company='Steele Group', City='Chavezborough', Country='Bosnia and Herzegovina', Phone 1='(283)437-3886x88321', Phone 2='999-728-1637', Email='louis27@gilbert.com', Subscription Date=datetime.date(2020, 2, 25), Website='http://www.berger.net/')\n",
            "Row(Index=7, Customer Id='EA4d384DfDbBf77', First Name='Darren', Last Name='Peck', Company='Lester, Woodard and Mitchell', City='Lake Ana', Country='Pitcairn Islands', Phone 1='(496)452-6181x3291', Phone 2='+1-247-266-0963x4995', Email='tgates@cantrell.com', Subscription Date=datetime.date(2021, 8, 24), Website='https://www.le.com/')\n",
            "Row(Index=8, Customer Id='0e04AFde9f225dE', First Name='Brett', Last Name='Mullen', Company='Sanford, Davenport and Giles', City='Kimport', Country='Bulgaria', Phone 1='001-583-352-7197x297', Phone 2='001-333-145-0369', Email='asnow@colon.com', Subscription Date=datetime.date(2021, 4, 12), Website='https://hammond-ramsey.com/')\n",
            "Row(Index=9, Customer Id='C2dE4dEEc489ae0', First Name='Sheryl', Last Name='Meyers', Company='Browning-Simon', City='Robersonstad', Country='Cyprus', Phone 1='854-138-4911x5772', Phone 2='+1-448-910-2276x729', Email='mariokhan@ryan-pope.org', Subscription Date=datetime.date(2020, 1, 13), Website='https://www.bullock.net/')\n",
            "Row(Index=10, Customer Id='8C2811a503C7c5a', First Name='Michelle', Last Name='Gallagher', Company='Beck-Hendrix', City='Elaineberg', Country='Timor-Leste', Phone 1='739.218.2516x459', Phone 2='001-054-401-0347x617', Email='mdyer@escobar.net', Subscription Date=datetime.date(2021, 11, 8), Website='https://arias.com/')\n"
          ]
        }
      ],
      "source": [
        "print(\"First five records of customer data set\", customers.take(5))\n",
        "print(\"Not so pretty....\")\n",
        "print(\"Now let us pretty print:\")\n",
        "# To pretty print, you need to iterate\n",
        "for element in customers.take(10):\n",
        "    print(element)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8HxXONGuoxA"
      },
      "source": [
        "###Key Operations\n",
        "\n",
        "PySpark examples for key based functions are groupByKey, reduceByKey, and sortByKey operations. Let us look at how they work.\n",
        "\n",
        "####groupByKey\n",
        "This operation groups the values for each key in the RDD into a single sequence.\n",
        "####reduceByKey\n",
        "This operation merges the values for each key using an associative reduce function.\n",
        "####sortByKey\n",
        "This operation sorts the dataset by keys.\n",
        "\n",
        "Let us put together an example to compare and contrast\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf_-cHMhvD0V",
        "outputId": "4587c068-dbd1-4dd1-8e64-ba8a42e01ef3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3: (6, 4)\n",
            "1: (2,)\n",
            "[(3, 10), (1, 2)]\n",
            "[(1, 2), (3, 6), (3, 4)]\n"
          ]
        }
      ],
      "source": [
        "rdd = spark.sparkContext.parallelize([(3, 6),(1, 2),(3, 4)])\n",
        "grouped = rdd.groupByKey()\n",
        "for key, values in grouped.collect():\n",
        "    print(f\"{key}: {tuple(values)}\")\n",
        "reduced = rdd.reduceByKey(lambda a, b: a + b)\n",
        "print(reduced.collect())\n",
        "sorted_rdd = rdd.sortByKey()\n",
        "print(sorted_rdd.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SETKMpJDwAZK"
      },
      "source": [
        "Please note that these operations are transformations and require an action like collect to retrieve the data. Also, keep in mind that groupByKey can cause a lot of data shuffling over the network, and it's generally more efficient to use reduceByKey where possible because it combines output values locally before sending data over the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-XSy6Lqwt4H"
      },
      "source": [
        "### Sampling\n",
        "\n",
        " The sample() transformation is used to sample a fraction of the data from an RDD. You can sample with or without replacement. Here's how you can use it:\n",
        "\n",
        "####Sampling without replacement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rfRqPNOw3Ft",
        "outputId": "ff423083-d641-4d1f-af0b-1824702ad9f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row(Index=1, Customer Id='DD37Cf93aecA6Dc', First Name='Sheryl', Last Name='Baxter', Company='Rasmussen Group', City='East Leonard', Country='Chile', Phone 1='229.077.5154', Phone 2='397.884.0519x718', Email='zunigavanessa@smith.info', Subscription Date=datetime.date(2020, 8, 24), Website='http://www.stephenson.com/')\n",
            "Row(Index=2, Customer Id='1Ef7b82A4CAAD10', First Name='Preston', Last Name='Lozano', Company='Vega-Gentry', City='East Jimmychester', Country='Djibouti', Phone 1='5153435776', Phone 2='686-620-1820x944', Email='vmata@colon.com', Subscription Date=datetime.date(2021, 4, 23), Website='http://www.hobbs.com/')\n",
            "Row(Index=14, Customer Id='A08A8aF8BE9FaD4', First Name='Kristine', Last Name='Cox', Company='Carpenter-Cook', City='Jodyberg', Country='Sri Lanka', Phone 1='786-284-3358x62152', Phone 2='+1-315-627-1796x8074', Email='holdenmiranda@clarke.com', Subscription Date=datetime.date(2021, 2, 8), Website='https://www.brandt.com/')\n",
            "Row(Index=15, Customer Id='6fEaA1b7cab7B6C', First Name='Faith', Last Name='Lutz', Company='Carter-Hancock', City='Burchbury', Country='Singapore', Phone 1='(781)861-7180x8306', Phone 2='207-185-3665', Email='cassieparrish@blevins-chapman.net', Subscription Date=datetime.date(2022, 1, 26), Website='http://stevenson.org/')\n",
            "Row(Index=22, Customer Id='FBd0Ded4F02a742', First Name='Kiara', Last Name='Houston', Company='Manning, Hester and Arroyo', City='South Alvin', Country='Netherlands', Phone 1='001-274-040-3582x10611', Phone 2='+1-528-175-0973x4684', Email='blanchardbob@wallace-shannon.com', Subscription Date=datetime.date(2020, 9, 15), Website='https://www.reid-potts.com/')\n",
            "Row(Index=23, Customer Id='2FB0FAA1d429421', First Name='Colleen', Last Name='Howard', Company='Greer and Sons', City='Brittanyview', Country='Paraguay', Phone 1='1935085151', Phone 2='(947)115-7711x5488', Email='rsingleton@ryan-cherry.com', Subscription Date=datetime.date(2020, 8, 19), Website='http://paul.biz/')\n",
            "Row(Index=26, Customer Id='09D7D7C8Fe09aea', First Name='Marcus', Last Name='Moody', Company='Giles Ltd', City='Kaitlyntown', Country='Panama', Phone 1='674-677-8623', Phone 2='909-277-5485x566', Email='donnamullins@norris-barrett.org', Subscription Date=datetime.date(2022, 5, 24), Website='https://www.curry.com/')\n",
            "Row(Index=27, Customer Id='aBdfcF2c50b0bfD', First Name='Dakota', Last Name='Poole', Company='Simmons Group', City='Michealshire', Country='Belarus', Phone 1='(371)987-8576x4720', Phone 2='071-152-1376', Email='stacey67@fields.org', Subscription Date=datetime.date(2022, 2, 20), Website='https://sanford-wilcox.biz/')\n",
            "Row(Index=29, Customer Id='3B5dAAFA41AFa22', First Name='Stefanie', Last Name='Fitzpatrick', Company='Santana-Duran', City='Acevedoville', Country='Saint Vincent and the Grenadines', Phone 1='(752)776-3286', Phone 2='+1-472-021-4814x85074', Email='wterrell@clark.com', Subscription Date=datetime.date(2020, 7, 30), Website='https://meyers.com/')\n",
            "Row(Index=35, Customer Id='aA9BAFfBc3710fe', First Name='Faith', Last Name='Moon', Company='Waters, Chase and Aguilar', City='West Marthaburgh', Country='Bahamas', Phone 1='+1-586-217-0359x6317', Phone 2='+1-818-199-1403', Email='willistonya@randolph-baker.com', Subscription Date=datetime.date(2021, 11, 3), Website='https://spencer-charles.info/')\n",
            "Row(Index=50, Customer Id='80F33Fd2AcebF05', First Name='Latoya', Last Name='Mccann', Company='Hobbs, Garrett and Sanford', City='Port Sergiofort', Country='Belarus', Phone 1='(530)287-4548x29481', Phone 2='162-234-0249x32790', Email='bobhammond@barry.biz', Subscription Date=datetime.date(2021, 12, 2), Website='https://www.burton.com/')\n",
            "Row(Index=59, Customer Id='8c7DdF10798bCC3', First Name='Kathy', Last Name='Hill', Company='Moore, Mccoy and Glass', City='Selenabury', Country='South Georgia and the South Sandwich Islands', Phone 1='001-171-716-2175x310', Phone 2='888.625.0654', Email='ncamacho@boone-simmons.org', Subscription Date=datetime.date(2020, 11, 15), Website='http://hayden.com/')\n",
            "Row(Index=64, Customer Id='FCBdfCEAe20A8Dc', First Name='Chloe', Last Name='Hutchinson', Company='Simon LLC', City='South Julia', Country='Netherlands', Phone 1='981-544-9452', Phone 2='+1-288-552-4666x060', Email='leah85@sutton-terrell.com', Subscription Date=datetime.date(2022, 5, 15), Website='https://mitchell.info/')\n",
            "Row(Index=95, Customer Id='BE91A0bdcA49Bbc', First Name='Darrell', Last Name='Douglas', Company='Newton, Petersen and Mathis', City='Daisyborough', Country='Mali', Phone 1='001-084-845-9524x1777', Phone 2='001-769-564-6303', Email='grayjean@lowery-good.com', Subscription Date=datetime.date(2022, 2, 17), Website='https://banks.biz/')\n",
            "Row(Index=100, Customer Id='2354a0E336A91A1', First Name='Clarence', Last Name='Haynes', Company='Le, Nash and Cross', City='Judymouth', Country='Honduras', Phone 1='(753)813-6941', Phone 2='783.639.1472', Email='colleen91@faulkner.biz', Subscription Date=datetime.date(2020, 3, 11), Website='http://www.hatfield-saunders.net/')\n"
          ]
        }
      ],
      "source": [
        "# Now, use the sample function to take a random sample of about 10% of the customers without replacement\n",
        "sampled_customers_rdd = customers.sample(False, 0.1)\n",
        "\n",
        "# Collect the results\n",
        "sampled_customers = sampled_customers_rdd.collect()\n",
        "\n",
        "# Print the sampled list of customers\n",
        "for customer in sampled_customers:\n",
        "    print(customer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZYw0oFExAcm"
      },
      "source": [
        "In the sample method:\n",
        "\n",
        "1. The first argument is withReplacement. Set it to False for sampling without replacement, meaning a particular customer can be chosen only once.\n",
        "2. The second argument is the fraction of the data to sample, which is 0.1 in this case, meaning approximately 10% of the data.\n",
        "\n",
        "This will output a random sample of the customers from your customers_rdd. The collect() action is used here for demonstration purposes, and it should be used with caution if the dataset is large, as it will gather all the sampled data to the driver node.\n",
        "\n",
        "####Sampling with replacement\n",
        "\n",
        "The following example shows how to use sample() with replacement. This means an element can be included in the sample multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzQm9EbUxEq4",
        "outputId": "34ab5254-e1e9-4137-ea23-f34542ef6612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row(Index=20, Customer Id='0F60FF3DdCd7aB0', First Name='Joanna', Last Name='Kirk', Company='Mays-Mccormick', City='Jamesshire', Country='French Polynesia', Phone 1='(266)131-7001x711', Phone 2='(283)312-5579x11543', Email='tuckerangie@salazar.net', Subscription Date=datetime.date(2021, 9, 24), Website='https://www.camacho.net/')\n",
            "Row(Index=70, Customer Id='CC68FD1D3Bbbf22', First Name='Riley', Last Name='Good', Company='Wade PLC', City='Erikaville', Country='Canada', Phone 1='6977745822', Phone 2='855-436-7641', Email='alex06@galloway.com', Subscription Date=datetime.date(2020, 2, 3), Website='http://conway.org/')\n"
          ]
        }
      ],
      "source": [
        "# Now, use the sample function to take a random sample of about 10% of the customers with replacement\n",
        "sampled_customers_rdd = customers.sample(True, 0.01)\n",
        "\n",
        "# Collect the results\n",
        "sampled_customers = sampled_customers_rdd.collect()\n",
        "\n",
        "# Print the sampled list of customers\n",
        "for customer in sampled_customers:\n",
        "    print(customer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJGdC0dfxpg4"
      },
      "source": [
        "These examples will give you an array of customers sampled from the original RDD. The actual elements in the sample will vary each time you run the code due to the randomness of the sampling process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYMoA6iZx7kL"
      },
      "source": [
        "### More Transformations\n",
        "In PySpark, you can perform various RDD operations such as union, join, and cartesian (cross) to combine data in different ways. Here are simple examples for each:\n",
        "#### Union\n",
        "The union operation combines two RDDs to form a new RDD that contains elements from both RDDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr-ClfANyEsJ",
        "outputId": "91063f25-63bd-45d2-bd0f-df5d56705f0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Alice', 1), ('Bob', 2), ('Charlie', 3), ('David', 4)]\n"
          ]
        }
      ],
      "source": [
        "# Create two RDDs\n",
        "rdd1 = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2)])\n",
        "rdd2 = spark.sparkContext.parallelize([(\"Charlie\", 3), (\"David\", 4)])\n",
        "\n",
        "# Perform the union operation\n",
        "union_rdd = rdd1.union(rdd2)\n",
        "\n",
        "# Collect and print the results\n",
        "print(union_rdd.collect())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V_PebbQyQ2A"
      },
      "source": [
        "####Join\n",
        "The join operation combines two RDDs based on their key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsgtMDGHyVK6",
        "outputId": "6d088daf-21c1-41e7-edb8-3a806b3d7c92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Alice', ('Apple', 1)), ('Bob', ('Banana', 2))]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create two RDDs with common keys\n",
        "rdd3 = spark.sparkContext.parallelize([(\"Alice\", \"Apple\"), (\"Bob\", \"Banana\")])\n",
        "rdd4 = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2)])\n",
        "\n",
        "# Perform the join operation\n",
        "join_rdd = rdd3.join(rdd4)\n",
        "\n",
        "# Collect and print the results\n",
        "print(join_rdd.collect())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwcN4mg6ykED"
      },
      "source": [
        "####Cross or Catesian\n",
        "The cartesian operation returns all possible pairs of (a, b) where a is in the first RDD and b is in the second RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw643DrQypKe",
        "outputId": "a3b37b63-9692-4346-c7d6-c558e102f3bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')]\n"
          ]
        }
      ],
      "source": [
        "# Create two RDDs\n",
        "rdd5 = spark.sparkContext.parallelize([1, 2])\n",
        "rdd6 = spark.sparkContext.parallelize([\"a\", \"b\"])\n",
        "\n",
        "# Perform the cartesian operation\n",
        "cross_rdd = rdd5.cartesian(rdd6)\n",
        "\n",
        "# Collect and print the results\n",
        "print(cross_rdd.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cMQA5AkywVv"
      },
      "source": [
        "Please note that the cartesian operation can be very expensive in terms of computation and memory usage, especially with large datasets, because it forms all possible combinations of elements between the two RDDs.\n",
        "\n",
        "### More Actions\n",
        "\n",
        "####save\n",
        "Saving an RDD in PySpark can be done in a variety of formats. Common formats include saving as text files, sequence files, or other file-based data sources. Below are examples of how to save an RDD that contains customer data as a text file.\n",
        "\n",
        "But we will see about this action after the NoSQL lecture.\n",
        "\n",
        "End of Demo\n",
        "\n",
        "Thank you for the patient listening. üôèüåû"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/suriarasai/BEAD2024/blob/main/colab/04_Stock_Price_Analysis_using_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stock Price Analysis using Spark\n",
        "Stock price prediction is a vital part of financial analysis, and it can be a daunting task to obtain and process data to make accurate predictions. Apache Spark can be a powerful tool in building a data pipeline for stock price prediction.\n",
        "\n",
        "The goal of this case study is to utilise Apache Spark to analyse stock price data and gain insights into the trends in the company‚Äôs stock values over time.\n",
        "\n",
        "## Ingestion and Cleansing\n",
        "Data was collected from Yahoo Finance App.\n",
        "\n",
        "https://finance.yahoo.com/lookup\n",
        "\n",
        "The data for this case study is publicly available on Yahoo Finance, and it includes information about a company‚Äôs daily stock values from 2023.\n",
        "\n",
        "We will load the data shared v in My Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# # to read in data from a text file, first upload the data file into your google drive and then mount your google drive onto colab\n",
        "# from google.colab import drive\n",
        "# # to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True)\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us name the location for the stock files in the folder name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PySpark and Spark Session\n",
        "\n",
        "The first step involves installing pyspark.  The next step is to install findspark library.\n",
        "\n",
        "*Note: the --ignore-install flag is used to ignore previous installations and use the latest one built alongside the allocated cluster.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# # install pyspark using pip\n",
        "# !pip install --ignore-install -q pyspark\n",
        "# # install findspark using pip\n",
        "# !pip install --ignore-install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next step is to create an active spark session object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "# import yfinance as yf\n",
        "# stock_folder = \"/content/drive/MyDrive/data/StockData\"\n",
        "# # stock_prices_data.to_csv(\"aapl_stock_prices_data.csv\")\n",
        "\n",
        "# from pyspark.sql.functions import udf\n",
        "# from pyspark.sql.types import DateType\n",
        "# from datetime import datetime\n",
        "\n",
        "# from pyspark.sql.types import FloatType\n",
        "# from pyspark.sql.types import IntegerType\n",
        "# import pyspark.sql.functions as func\n",
        "# from pyspark.sql.window import Window\n",
        "\n",
        "# ## Reading CSV data => Stocks\n",
        "# stocks = spark.read.csv(stock_folder, header=True)\n",
        "\n",
        "# # import collections\n",
        "# spark = SparkSession.builder.master(\"local\").appName(\"Stock Ticker\").config('spark.ui.port', '4050').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us look at how to collect stock ticker data into  files. There are different data sources for obtaining stock data, such as Yahoo Finance, Google Finance, and Alpha Vantage.\n",
        "\n",
        "We will use Yahoo Financce. This code downloads Apple Inc. (AAPL) daily stock prices from January 1, 2020 to December 28, 2023 and saves the data in a CSV file called ‚Äúaapl_stock_prices_data.csv.‚Äù\n",
        "\n",
        "We can change the code to obtain data for a different firm or time period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "\n",
        "ticker = \"AAPL\"  # Replace with the ticker symbol of the company you want to analyze\n",
        "start_date = \"2020-01-01\"\n",
        "end_date = \"2023-12-28\"\n",
        "\n",
        "stock_prices_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "\n",
        "stock_prices_data.to_csv(\"aapl_stock_prices_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let us load all the stock files for this tutorial into a Dataframe by just pointing the folder name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us take a look at the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+----------+-------+--------+--------+--------+\n",
            "|Ticker|      Date|Close/Last| Volume|    Open|    High|     Low|\n",
            "+------+----------+----------+-------+--------+--------+--------+\n",
            "| BRK-B|05/31/2023|  $321.08 |6175417|$321.12 |$322.41 |$319.39 |\n",
            "| BRK-B|05/30/2023|  $322.19 |3232461|$321.86 |$322.47 |$319.00 |\n",
            "| BRK-B|05/26/2023|  $320.60 |3229873|$320.44 |$322.63 |$319.67 |\n",
            "| BRK-B|05/25/2023|  $319.02 |4251935|$320.56 |$320.56 |$317.71 |\n",
            "| BRK-B|05/24/2023|  $320.20 |3075393|$322.71 |$323.00 |$319.56 |\n",
            "+------+----------+----------+-------+--------+--------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Seeing Data => Dataframe\n",
        "stocks.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Ticker: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Close/Last: string (nullable = true)\n",
            " |-- Volume: string (nullable = true)\n",
            " |-- Open: string (nullable = true)\n",
            " |-- High: string (nullable = true)\n",
            " |-- Low: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Seeing Schema of the Data => Data Types in Dataframe\n",
        "stocks.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+\n",
            "|Ticker|\n",
            "+------+\n",
            "| BRK-B|\n",
            "| BRK-B|\n",
            "| BRK-B|\n",
            "+------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Basic select operation => Select Ticker, Date and Close price\n",
        "stocks.select(\"Ticker\").show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+--------+\n",
            "|Ticker|      Date|    Open|\n",
            "+------+----------+--------+\n",
            "| BRK-B|05/31/2023|$321.12 |\n",
            "| BRK-B|05/30/2023|$321.86 |\n",
            "| BRK-B|05/26/2023|$320.44 |\n",
            "| BRK-B|05/25/2023|$320.56 |\n",
            "| BRK-B|05/24/2023|$322.71 |\n",
            "+------+----------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stocks.select([\"Ticker\", \"Date\", \"Open\"]).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+----------+--------+--------+--------+--------+\n",
            "|Ticker|      Date|Close/Last|  Volume|    Open|    High|     Low|\n",
            "+------+----------+----------+--------+--------+--------+--------+\n",
            "|  MSFT|05/31/2023|  $328.39 |45950550|$332.29 |$335.94 |$327.33 |\n",
            "|  MSFT|05/30/2023|  $331.21 |29503070|$335.23 |$335.74 |$330.52 |\n",
            "|  MSFT|05/26/2023|  $332.89 |36630630|$324.02 |$333.40 |$323.88 |\n",
            "|  MSFT|05/25/2023|  $325.92 |43301740|$323.24 |$326.90 |$320.00 |\n",
            "|  MSFT|05/24/2023|  $313.85 |23384890|$314.73 |$316.50 |$312.61 |\n",
            "|  MSFT|05/23/2023|  $315.26 |30797170|$320.03 |$322.72 |$315.25 |\n",
            "|  MSFT|05/22/2023|  $321.18 |24115660|$318.60 |$322.59 |$318.01 |\n",
            "|  MSFT|05/19/2023|  $318.34 |27546700|$316.74 |$318.75 |$316.37 |\n",
            "|  MSFT|05/18/2023|  $318.52 |27275990|$314.53 |$319.04 |$313.72 |\n",
            "|  MSFT|05/17/2023|  $314.00 |24315010|$312.29 |$314.43 |$310.74 |\n",
            "+------+----------+----------+--------+--------+--------+--------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Filtering Data => Select rows containing Microsoft Stock in last one month\n",
        "stocks.filter(stocks.Ticker == \"MSFT\").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+----------+--------+--------+--------+--------+\n",
            "|Ticker|      Date|Close/Last|  Volume|    Open|    High|     Low|\n",
            "+------+----------+----------+--------+--------+--------+--------+\n",
            "|  MSFT|05/31/2023|  $328.39 |45950550|$332.29 |$335.94 |$327.33 |\n",
            "+------+----------+----------+--------+--------+--------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stocks.filter((stocks.Ticker == \"MSFT\") & (stocks.Date == \"05/31/2023\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+----------+--------+--------+--------+--------+\n",
            "|Ticker|      Date|Close/Last|  Volume|    Open|    High|     Low|\n",
            "+------+----------+----------+--------+--------+--------+--------+\n",
            "|  MSFT|05/31/2023|  $328.39 |45950550|$332.29 |$335.94 |$327.33 |\n",
            "|     V|05/31/2023|  $221.03 |20460620|$219.96 |$221.53 |$216.14 |\n",
            "+------+----------+----------+--------+--------+--------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stocks.filter(((stocks.Ticker == \"MSFT\") | (stocks.Ticker == \"V\")) & (stocks.Date == \"05/31/2023\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+----------+---------+--------+--------+--------+\n",
            "|Ticker|      Date|Close/Last|   Volume|    Open|    High|     Low|\n",
            "+------+----------+----------+---------+--------+--------+--------+\n",
            "|  MSFT|05/31/2023|  $328.39 | 45950550|$332.29 |$335.94 |$327.33 |\n",
            "|  TSLA|05/31/2023|  $203.93 |150711700|$199.78 |$203.95 |$195.12 |\n",
            "|     V|05/31/2023|  $221.03 | 20460620|$219.96 |$221.53 |$216.14 |\n",
            "|   SPY|05/31/2023|    417.85|110811800|  418.28|  419.22|  416.22|\n",
            "|   QQQ|05/31/2023|    347.99| 65105380|  348.37|   350.6|  346.51|\n",
            "+------+----------+----------+---------+--------+--------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stocks.filter((stocks.Ticker.isin([\"MSFT\", \"QQQ\", \"SPY\", \"V\", \"TSLA\"])) & (stocks.Date == \"05/31/2023\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Ticker: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Close/Last: string (nullable = true)\n",
            " |-- Volume: string (nullable = true)\n",
            " |-- Open: string (nullable = true)\n",
            " |-- High: string (nullable = true)\n",
            " |-- Low: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stocks.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Defining the User-Defined Function (UDF):\n",
        "\n",
        "1. date_parser: This is a UDF that takes a date string and converts it to a DateType. It's defined using Python's lambda function.\n",
        "2. lambda date: datetime.strptime(date, \"%m/%d/%Y\"): The lambda function takes a date string as input, and uses datetime.strptime to parse it into a datetime object based on the specified format (\"%m/%d/%Y\"). This format represents month/day/year, e.g., \"12/31/2023\".\n",
        "3. DateType(): The UDF is registered with a return type of DateType, meaning it will convert the output of the datetime.strptime function into a PySpark DateType.\n",
        "\n",
        "Applying the UDF to a DataFrame:\n",
        "1. stocks = stocks.withColumn(\"ParsedDate\", date_parser(stocks.Date)): This line applies the date_parser UDF to the Date column of the stocks DataFrame. It creates a new column, ParsedDate, which contains the parsed dates.\n",
        "2. withColumn: This method is used to add a new column or replace an existing column in a DataFrame. The first argument is the name of the new column, and the second argument is the column itself (in this case, generated by applying the UDF)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Ticker: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Close/Last: string (nullable = true)\n",
            " |-- Volume: string (nullable = true)\n",
            " |-- Open: string (nullable = true)\n",
            " |-- High: string (nullable = true)\n",
            " |-- Low: string (nullable = true)\n",
            " |-- ParsedDate: date (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## User Defined Functions\n",
        "\n",
        "date_parser = udf(lambda date: datetime.strptime(date,\"%m/%d/%Y\"), DateType())\n",
        "stocks = stocks.withColumn(\"ParsedDate\", date_parser(stocks.Date))\n",
        "stocks.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The UDF parser_number is used in DataFrame operations to process columns. For instance, if there is a DataFrame column with monetary values stored as strings like $100, parser_number can be used to convert these strings into floating-point numbers for further numerical processing.\n",
        "\n",
        "\n",
        "  The function num_parser is designed to take an input value and perform certain operations based on its type.\n",
        "  if isinstance(value, str): This checks if the value is a string. If so, it assumes the string represents a number with a dollar sign (e.g., \"$100\") and tries to convert it to a floating-point number.\n",
        "\n",
        "    value.strip(\"$\"): This removes the dollar sign from the string.\n",
        "    float(...): Converts the cleaned string to a floating-point number.\n",
        "  \n",
        "  elif isinstance(value, int) or isinstance(value, float): This checks if the value is already an integer or floating-point number. If so, it just returns the value as it is.\n",
        "  \n",
        "  else: If the value is neither a string, nor an int, nor a float, the function returns None, indicating an inability to process the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def num_parser(value):\n",
        "    if isinstance(value, str):\n",
        "        return float(value.strip(\"$\"))\n",
        "    elif isinstance(value, int) or isinstance(value, float):\n",
        "        return value\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "parser_number = udf(num_parser, FloatType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stocks = (stocks.withColumn(\"Open\", parser_number(stocks.Open))\n",
        "                .withColumn(\"Close\", parser_number(stocks[\"Close/Last\"]))\n",
        "                .withColumn(\"Low\", parser_number(stocks.Low))\n",
        "                .withColumn(\"High\", parser_number(stocks.High)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Ticker: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Close/Last: string (nullable = true)\n",
            " |-- Volume: string (nullable = true)\n",
            " |-- Open: float (nullable = true)\n",
            " |-- High: float (nullable = true)\n",
            " |-- Low: float (nullable = true)\n",
            " |-- ParsedDate: date (nullable = true)\n",
            " |-- Close: float (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stocks.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parse_int = udf(lambda value: int(value), IntegerType())\n",
        "## Changing the datatype of the column\n",
        "stocks = stocks.withColumn(\"Volume\", parse_int(stocks.Volume))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Ticker: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Close/Last: string (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Open: float (nullable = true)\n",
            " |-- High: float (nullable = true)\n",
            " |-- Low: float (nullable = true)\n",
            " |-- ParsedDate: date (nullable = true)\n",
            " |-- Close: float (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stocks.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+-------+------+------+------+------+\n",
            "|Ticker|ParsedDate| Volume|  Open|   Low|  High| Close|\n",
            "+------+----------+-------+------+------+------+------+\n",
            "| BRK-B|2023-05-31|6175417|321.12|319.39|322.41|321.08|\n",
            "| BRK-B|2023-05-30|3232461|321.86| 319.0|322.47|322.19|\n",
            "| BRK-B|2023-05-26|3229873|320.44|319.67|322.63| 320.6|\n",
            "| BRK-B|2023-05-25|4251935|320.56|317.71|320.56|319.02|\n",
            "| BRK-B|2023-05-24|3075393|322.71|319.56| 323.0| 320.2|\n",
            "+------+----------+-------+------+------+------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cleaned_stocks = stocks.select([\"Ticker\", \"ParsedDate\", \"Volume\", \"Open\", \"Low\", \"High\", \"Close\"])\n",
        "cleaned_stocks.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------------------+------------------+------------------+------------------+------------------+\n",
            "|summary|              Volume|              Open|               Low|              High|             Close|\n",
            "+-------+--------------------+------------------+------------------+------------------+------------------+\n",
            "|  count|               15108|             15108|             15108|             15108|             15108|\n",
            "|   mean|5.1868408793685466E7|180.09656566181036| 177.9982781513109| 182.1253348687101| 180.1256089860054|\n",
            "| stddev| 5.496484129953461E7|101.16125813324406|100.26590135955226|101.96625521621772|101.14891782168523|\n",
            "|    min|              961133|             12.07|              11.8|             12.45|             11.93|\n",
            "|    max|           914080943|            479.22|            476.06|            479.98|            477.71|\n",
            "+-------+--------------------+------------------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Calculating basic stastics about data => Calculate average stock price\n",
        "cleaned_stocks.describe([\"Volume\", \"Open\", \"Low\", \"High\", \"Close\"]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Stock Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+---------+\n",
            "|Ticker|max(Open)|\n",
            "+------+---------+\n",
            "| BRK-B|   361.39|\n",
            "|   TSM|   141.61|\n",
            "|  AAPL|   182.63|\n",
            "|  META|   381.68|\n",
            "|  TSLA|   411.47|\n",
            "|   QQQ|   405.57|\n",
            "|     V|   250.05|\n",
            "| GOOGL|   151.25|\n",
            "|   SPY|   479.22|\n",
            "|  AMZN|    187.2|\n",
            "|  MSFT|   344.62|\n",
            "|  NVDA|   405.95|\n",
            "+------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Calculate maximum stock price for various stocks\n",
        "cleaned_stocks.groupBy(\"Ticker\").max(\"Open\").show(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------------+\n",
            "|Ticker|MaxStockPrice|\n",
            "+------+-------------+\n",
            "| BRK-B|       361.39|\n",
            "|   TSM|       141.61|\n",
            "|  AAPL|       182.63|\n",
            "|  META|       381.68|\n",
            "|  TSLA|       411.47|\n",
            "|   QQQ|       405.57|\n",
            "|     V|       250.05|\n",
            "| GOOGL|       151.25|\n",
            "|   SPY|       479.22|\n",
            "|  AMZN|        187.2|\n",
            "|  MSFT|       344.62|\n",
            "|  NVDA|       405.95|\n",
            "+------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "cleaned_stocks.groupBy(\"Ticker\").max(\"Open\").withColumnRenamed(\"max(Open)\", \"MaxStockPrice\").show(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------------+\n",
            "|Ticker|MaxStockPrice|\n",
            "+------+-------------+\n",
            "| BRK-B|       361.39|\n",
            "|   TSM|       141.61|\n",
            "|  AAPL|       182.63|\n",
            "|  META|       381.68|\n",
            "|  TSLA|       411.47|\n",
            "|   QQQ|       405.57|\n",
            "|     V|       250.05|\n",
            "| GOOGL|       151.25|\n",
            "|   SPY|       479.22|\n",
            "|  AMZN|        187.2|\n",
            "|  MSFT|       344.62|\n",
            "|  NVDA|       405.95|\n",
            "+------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cleaned_stocks.groupBy(\"Ticker\").agg(func.max(\"Open\").alias(\"MaxStockPrice\")).show(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------------+------------+\n",
            "|Ticker|MaxStockPrice| TotalVolume|\n",
            "+------+-------------+------------+\n",
            "| BRK-B|       361.39|  5862401321|\n",
            "|   TSM|       141.61| 12506470104|\n",
            "|  AAPL|       182.63|139310061360|\n",
            "|  META|       381.68| 30148848043|\n",
            "|  TSLA|       411.47|171802975076|\n",
            "|   QQQ|       405.57| 60437153773|\n",
            "|     V|       250.05| 10410997871|\n",
            "| GOOGL|       151.25| 43956560981|\n",
            "|   SPY|       479.22|107925285300|\n",
            "|  AMZN|        187.2|104503287430|\n",
            "|  MSFT|       344.62| 37976660472|\n",
            "|  NVDA|       405.95| 58787218324|\n",
            "+------+-------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cleaned_stocks.groupBy(\"Ticker\").agg(\n",
        "    func.max(\"Open\").alias(\"MaxStockPrice\"),\n",
        "    func.sum(\"Volume\").alias(\"TotalVolume\")\n",
        ").show(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code starts with cleaned_stocks, which is a DataFrame that has already undergone preprocessing, including the parsing of dates.\n",
        "\n",
        "By extracting the year, month, day, and week information from the date, the DataFrame is enhanced for more detailed analysis. For instance, you can now easily group or filter the data based on these time components.\n",
        "\n",
        "This kind of manipulation is particularly useful in time series analysis, such as calculating the maximum price of stocks in each year, analyzing monthly trends, or understanding weekly patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+-------+------+------+------+------+----+-----+---+----+\n",
            "|Ticker|ParsedDate| Volume|  Open|   Low|  High| Close|Year|Month|Day|Week|\n",
            "+------+----------+-------+------+------+------+------+----+-----+---+----+\n",
            "| BRK-B|2023-05-31|6175417|321.12|319.39|322.41|321.08|2023|    5| 31|  22|\n",
            "| BRK-B|2023-05-30|3232461|321.86| 319.0|322.47|322.19|2023|    5| 30|  22|\n",
            "| BRK-B|2023-05-26|3229873|320.44|319.67|322.63| 320.6|2023|    5| 26|  21|\n",
            "| BRK-B|2023-05-25|4251935|320.56|317.71|320.56|319.02|2023|    5| 25|  21|\n",
            "| BRK-B|2023-05-24|3075393|322.71|319.56| 323.0| 320.2|2023|    5| 24|  21|\n",
            "| BRK-B|2023-05-23|4031342|328.19|322.97|329.27|323.11|2023|    5| 23|  21|\n",
            "| BRK-B|2023-05-22|2763422|330.75|328.35|331.49|329.13|2023|    5| 22|  21|\n",
            "| BRK-B|2023-05-19|4323538| 331.0|329.12|333.94|330.39|2023|    5| 19|  20|\n",
            "| BRK-B|2023-05-18|2808329|326.87|325.85|329.98|329.76|2023|    5| 18|  20|\n",
            "| BRK-B|2023-05-17|3047626|325.02|324.82|328.26|327.39|2023|    5| 17|  20|\n",
            "+------+----------+-------+------+------+------+------+----+-----+---+----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Calculate maximum price of stocks each year => Basic date manipulation operation\n",
        "cleaned_stocks = (cleaned_stocks.withColumn(\"Year\", func.year(cleaned_stocks.ParsedDate))\n",
        "                                .withColumn(\"Month\", func.month(cleaned_stocks.ParsedDate))\n",
        "                                .withColumn(\"Day\", func.dayofmonth(cleaned_stocks.ParsedDate))\n",
        "                                .withColumn(\"Week\", func.weekofyear(cleaned_stocks.ParsedDate))\n",
        "                 )\n",
        "cleaned_stocks.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----+-----+---------+--------+\n",
            "|Ticker|Year|Month|MonthHigh|MonthLow|\n",
            "+------+----+-----+---------+--------+\n",
            "| BRK-B|2022|   10|   297.98|  260.58|\n",
            "|  META|2020|    6|   241.28|  209.75|\n",
            "| BRK-B|2018|    9|   222.13|  209.21|\n",
            "|  MSFT|2022|    6|    275.2|  243.86|\n",
            "|  MSFT|2021|    2|   245.03|  230.01|\n",
            "|     V|2020|    7|    200.0|   189.5|\n",
            "|   SPY|2019|    6|   296.04|  275.31|\n",
            "|   QQQ|2019|    6|   189.72|  171.98|\n",
            "|   QQQ|2019|    2|    174.2|  166.73|\n",
            "|  MSFT|2020|    1|   174.05|  157.08|\n",
            "| BRK-B|2021|   10|   290.85|  273.02|\n",
            "| BRK-B|2020|   10|   216.74|  200.03|\n",
            "|  TSLA|2023|    4|   199.91|  152.64|\n",
            "|  TSLA|2019|    4|    19.22|   15.72|\n",
            "|  AMZN|2018|    5|    81.15|   81.15|\n",
            "|     V|2020|    4|   181.78|  152.53|\n",
            "| GOOGL|2018|    5|     54.1|    54.1|\n",
            "|  NVDA|2021|    3|    139.0|   121.3|\n",
            "|  NVDA|2019|    6|    41.25|   33.98|\n",
            "|     V|2018|    7|   143.08|  131.96|\n",
            "+------+----+-----+---------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Calculate average stock price for stock each month\n",
        "monthly = cleaned_stocks.groupBy(['Ticker', 'Year', 'Month']).agg(func.max(\"Open\").alias(\"MonthHigh\"), func.min(\"Open\").alias(\"MonthLow\"))\n",
        "weekly = cleaned_stocks.groupBy(['Ticker', 'Year', 'Week']).agg(func.max(\"Open\").alias(\"WeekHigh\"), func.min(\"Open\").alias(\"WeekLow\"))\n",
        "monthly.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----+----+--------+-------+\n",
            "|Ticker|Year|Week|WeekHigh|WeekLow|\n",
            "+------+----+----+--------+-------+\n",
            "| BRK-B|2022|  14|   352.0| 341.17|\n",
            "| BRK-B|2022|  10|  326.59| 322.49|\n",
            "| BRK-B|2021|  14|  264.22| 260.02|\n",
            "|  META|2022|  43|  131.68|  97.98|\n",
            "|  META|2020|   6|  212.51| 203.44|\n",
            "|  TSLA|2022|  20|  255.72| 235.67|\n",
            "|  TSLA|2020|  19|   52.92|  46.73|\n",
            "|  TSLA|2020|  16|   51.49|  39.34|\n",
            "|  TSLA|2018|  39|   20.86|  18.02|\n",
            "| GOOGL|2021|  29|  130.43| 125.53|\n",
            "|  NVDA|2020|  37|  129.89| 117.35|\n",
            "|   SPY|2022|  42|  375.13| 364.01|\n",
            "|   SPY|2021|  52|  477.93| 472.06|\n",
            "|   QQQ|2022|  34|  320.28| 313.61|\n",
            "|   QQQ|2022|  29|  306.43| 293.11|\n",
            "|   QQQ|2021|  45|  399.16| 391.77|\n",
            "| BRK-B|2018|  48|  217.23|  209.3|\n",
            "|  MSFT|2022|   6|  309.87| 301.25|\n",
            "|  MSFT|2021|   2|  218.47| 213.52|\n",
            "|  META|2022|  40|  140.49| 136.76|\n",
            "+------+----+----+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "weekly.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----+----+--------+-------+---------+\n",
            "|Ticker|Year|Week|WeekHigh|WeekLow|   Spread|\n",
            "+------+----+----+--------+-------+---------+\n",
            "| BRK-B|2022|  14|   352.0| 341.17|10.829987|\n",
            "| BRK-B|2022|  10|  326.59| 322.49| 4.100006|\n",
            "| BRK-B|2021|  14|  264.22| 260.02| 4.200012|\n",
            "|  META|2022|  43|  131.68|  97.98| 33.69999|\n",
            "|  META|2020|   6|  212.51| 203.44| 9.069992|\n",
            "|  TSLA|2022|  20|  255.72| 235.67|20.050003|\n",
            "|  TSLA|2020|  19|   52.92|  46.73|6.1899986|\n",
            "|  TSLA|2020|  16|   51.49|  39.34|12.150002|\n",
            "|  TSLA|2018|  39|   20.86|  18.02|2.8400002|\n",
            "| GOOGL|2021|  29|  130.43| 125.53| 4.899994|\n",
            "|  NVDA|2020|  37|  129.89| 117.35|12.540001|\n",
            "|   SPY|2022|  42|  375.13| 364.01|11.119995|\n",
            "|   SPY|2021|  52|  477.93| 472.06| 5.869995|\n",
            "|   QQQ|2022|  34|  320.28| 313.61|6.6700134|\n",
            "|   QQQ|2022|  29|  306.43| 293.11|13.320007|\n",
            "|   QQQ|2021|  45|  399.16| 391.77|7.3900146|\n",
            "| BRK-B|2018|  48|  217.23|  209.3|7.9299927|\n",
            "|  MSFT|2022|   6|  309.87| 301.25| 8.619995|\n",
            "|  MSFT|2021|   2|  218.47| 213.52| 4.949997|\n",
            "|  META|2022|  40|  140.49| 136.76| 3.730011|\n",
            "+------+----+----+--------+-------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "weekly.withColumn(\"Spread\", weekly['WeekHigh'] - weekly['WeekLow']).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----+---------+---------+\n",
            "|Ticker|Year|YearlHigh|YearlyLow|\n",
            "+------+----+---------+---------+\n",
            "|  NVDA|2020|   147.04|    50.02|\n",
            "|     V|2022|   233.04|    175.0|\n",
            "|  META|2020|   300.16|   139.75|\n",
            "| BRK-B|2023|    331.0|   294.68|\n",
            "|   TSM|2018|     45.0|    35.33|\n",
            "|   TSM|2021|   141.61|    108.0|\n",
            "|   SPY|2022|   479.22|  349.205|\n",
            "|  MSFT|2019|   159.45|    99.55|\n",
            "|  MSFT|2021|   344.62|   212.17|\n",
            "| BRK-B|2018|    224.0|   185.43|\n",
            "|  META|2021|   381.68|    247.9|\n",
            "|  TSLA|2019|     29.0|    12.07|\n",
            "|   QQQ|2020|   314.16|   170.92|\n",
            "|  META|2018|   215.72|    123.1|\n",
            "|  AAPL|2020|   138.05|    57.02|\n",
            "|  MSFT|2020|   229.27|   137.01|\n",
            "|  TSLA|2021|   411.47|   184.18|\n",
            "|     V|2021|   250.05|    192.0|\n",
            "|  NVDA|2018|    72.33|    31.62|\n",
            "|   SPY|2020|   373.81|   228.19|\n",
            "+------+----+---------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "yearly = cleaned_stocks.groupBy(['Ticker', 'Year']).agg(func.max(\"Open\").alias(\"YearlHigh\"), func.min(\"Open\").alias(\"YearlyLow\"))\n",
        "yearly.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Join\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code snippet you provided demonstrates the use of a join operation, which combines two DataFrames based on specified conditions. Let's dissect this particular join operation:\n",
        "\n",
        "*   cleaned_stocks: This is a  DataFrame that contains stock-related data. It has columns such as Ticker, Year, and other stock-related information.\n",
        "*   yearly: This is another DataFrame. It contains yearly data related to stocks and also includes Ticker and Year columns for reference.\n",
        "\n",
        "An inner join will result in a DataFrame that only includes rows where there is a match in both cleaned_stocks and yearly DataFrames based on the specified conditions.\n",
        "\n",
        "This type of join is useful in scenarios where you need to combine data from two different sources based on common identifiers. In this case, it's combining stock information based on matching stock tickers and years."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+-------+------+------+------+------+----+-----+---+----+------+----+---------+---------+\n",
            "|Ticker|ParsedDate| Volume|  Open|   Low|  High| Close|Year|Month|Day|Week|Ticker|Year|YearlHigh|YearlyLow|\n",
            "+------+----------+-------+------+------+------+------+----+-----+---+----+------+----+---------+---------+\n",
            "| BRK-B|2023-05-31|6175417|321.12|319.39|322.41|321.08|2023|    5| 31|  22| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-30|3232461|321.86| 319.0|322.47|322.19|2023|    5| 30|  22| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-26|3229873|320.44|319.67|322.63| 320.6|2023|    5| 26|  21| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-25|4251935|320.56|317.71|320.56|319.02|2023|    5| 25|  21| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-24|3075393|322.71|319.56| 323.0| 320.2|2023|    5| 24|  21| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-23|4031342|328.19|322.97|329.27|323.11|2023|    5| 23|  21| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-22|2763422|330.75|328.35|331.49|329.13|2023|    5| 22|  21| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-19|4323538| 331.0|329.12|333.94|330.39|2023|    5| 19|  20| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-18|2808329|326.87|325.85|329.98|329.76|2023|    5| 18|  20| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-17|3047626|325.02|324.82|328.26|327.39|2023|    5| 17|  20| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-16|2139996|322.46|322.36|324.69|323.75|2023|    5| 16|  20| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-15|2191609|322.89|320.13|323.83|323.53|2023|    5| 15|  20| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-12|1938264|323.82|320.54|324.24|322.49|2023|    5| 12|  19| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-11|2549339| 321.0|319.81|322.96|322.64|2023|    5| 11|  19| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-10|2641134|326.08|320.15|326.16|322.99|2023|    5| 10|  19| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-09|2285924|324.87|323.48|326.88|324.87|2023|    5|  9|  19| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-08|3303393|328.26|325.79|330.69|326.14|2023|    5|  8|  19| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-05|3876299|323.36|322.62|325.16|323.88|2023|    5|  5|  18| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-04|3194768|323.44|317.41|325.99| 320.0|2023|    5|  4|  18| BRK-B|2023|    331.0|   294.68|\n",
            "| BRK-B|2023-05-03|2660456|327.13|323.06|328.07|323.22|2023|    5|  3|  18| BRK-B|2023|    331.0|   294.68|\n",
            "+------+----------+-------+------+------+------+------+----+-----+---+----+------+----+---------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Joins\n",
        "cleaned_stocks.join(yearly, (cleaned_stocks.Ticker==yearly.Ticker) & (cleaned_stocks.Year == yearly.Year),\n",
        "                                      'inner'\n",
        "                                     ).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------+------+------+------+------+-----+---+----+------+----+---------+---------+\n",
            "|ParsedDate| Volume|  Open|   Low|  High| Close|Month|Day|Week|Ticker|Year|YearlHigh|YearlyLow|\n",
            "+----------+-------+------+------+------+------+-----+---+----+------+----+---------+---------+\n",
            "|2023-05-31|6175417|321.12|319.39|322.41|321.08|    5| 31|  22| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-30|3232461|321.86| 319.0|322.47|322.19|    5| 30|  22| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-26|3229873|320.44|319.67|322.63| 320.6|    5| 26|  21| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-25|4251935|320.56|317.71|320.56|319.02|    5| 25|  21| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-24|3075393|322.71|319.56| 323.0| 320.2|    5| 24|  21| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-23|4031342|328.19|322.97|329.27|323.11|    5| 23|  21| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-22|2763422|330.75|328.35|331.49|329.13|    5| 22|  21| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-19|4323538| 331.0|329.12|333.94|330.39|    5| 19|  20| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-18|2808329|326.87|325.85|329.98|329.76|    5| 18|  20| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-17|3047626|325.02|324.82|328.26|327.39|    5| 17|  20| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-16|2139996|322.46|322.36|324.69|323.75|    5| 16|  20| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-15|2191609|322.89|320.13|323.83|323.53|    5| 15|  20| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-12|1938264|323.82|320.54|324.24|322.49|    5| 12|  19| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-11|2549339| 321.0|319.81|322.96|322.64|    5| 11|  19| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-10|2641134|326.08|320.15|326.16|322.99|    5| 10|  19| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-09|2285924|324.87|323.48|326.88|324.87|    5|  9|  19| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-08|3303393|328.26|325.79|330.69|326.14|    5|  8|  19| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-05|3876299|323.36|322.62|325.16|323.88|    5|  5|  18| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-04|3194768|323.44|317.41|325.99| 320.0|    5|  4|  18| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-03|2660456|327.13|323.06|328.07|323.22|    5|  3|  18| BRK-B|2023|    331.0|   294.68|\n",
            "+----------+-------+------+------+------+------+-----+---+----+------+----+---------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cleaned_stocks.join(yearly,\n",
        "                    (cleaned_stocks.Ticker==yearly.Ticker) & (cleaned_stocks.Year == yearly.Year),\n",
        "                    'inner'\n",
        "                   ).drop(yearly.Year, yearly.Ticker).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "historic_stocks = cleaned_stocks.join(yearly,\n",
        "                    (cleaned_stocks.Ticker==yearly.Ticker) & (cleaned_stocks.Year == yearly.Year),\n",
        "                    'inner'\n",
        "                   ).drop(yearly.Year, yearly.Ticker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------+------+------+------+------+-----+---+----+------+----+---------+---------+\n",
            "|ParsedDate| Volume|  Open|   Low|  High| Close|Month|Day|Week|Ticker|Year|YearlHigh|YearlyLow|\n",
            "+----------+-------+------+------+------+------+-----+---+----+------+----+---------+---------+\n",
            "|2023-05-31|6175417|321.12|319.39|322.41|321.08|    5| 31|  22| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-30|3232461|321.86| 319.0|322.47|322.19|    5| 30|  22| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-26|3229873|320.44|319.67|322.63| 320.6|    5| 26|  21| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-25|4251935|320.56|317.71|320.56|319.02|    5| 25|  21| BRK-B|2023|    331.0|   294.68|\n",
            "|2023-05-24|3075393|322.71|319.56| 323.0| 320.2|    5| 24|  21| BRK-B|2023|    331.0|   294.68|\n",
            "+----------+-------+------+------+------+------+-----+---+----+------+----+---------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "historic_stocks.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Analysis\n",
        "In essence, this code demonstrates how create a new column in the snapshot DataFrame that contains the previous day's opening price for each stock. This is done using a window function that partitions the data by stock ticker and orders it by date. The lag function is then applied to get the opening price from the previous day within each partition. This kind of operation is common in time series analysis, particularly with financial data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+------+\n",
            "|Ticker|ParsedDate|  Open|\n",
            "+------+----------+------+\n",
            "| BRK-B|2023-05-31|321.12|\n",
            "| BRK-B|2023-05-30|321.86|\n",
            "| BRK-B|2023-05-26|320.44|\n",
            "| BRK-B|2023-05-25|320.56|\n",
            "| BRK-B|2023-05-24|322.71|\n",
            "| BRK-B|2023-05-23|328.19|\n",
            "| BRK-B|2023-05-22|330.75|\n",
            "| BRK-B|2023-05-19| 331.0|\n",
            "| BRK-B|2023-05-18|326.87|\n",
            "| BRK-B|2023-05-17|325.02|\n",
            "| BRK-B|2023-05-16|322.46|\n",
            "| BRK-B|2023-05-15|322.89|\n",
            "| BRK-B|2023-05-12|323.82|\n",
            "| BRK-B|2023-05-11| 321.0|\n",
            "| BRK-B|2023-05-10|326.08|\n",
            "| BRK-B|2023-05-09|324.87|\n",
            "| BRK-B|2023-05-08|328.26|\n",
            "| BRK-B|2023-05-05|323.36|\n",
            "| BRK-B|2023-05-04|323.44|\n",
            "| BRK-B|2023-05-03|327.13|\n",
            "+------+----------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "snapshot = cleaned_stocks.select(['Ticker', 'ParsedDate', 'Open'])\n",
        "snapshot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Window functions are used to perform calculations across a set of rows that are somehow related to the current row, such as rows within the same group.\n",
        "\n",
        "Window partitionBy Ticker: This partitions the data by the Ticker column. In the context of stock data, each Ticker represents a different stock, so partitioning by Ticker means that each stock will be treated separately. ParsedDate is the column representing the date associated with each record.\n",
        "\n",
        "The lag function is used to access data from a previous row. In this case, it's accessing the value from the Open column in the row preceding the current row (lag of 1). The Open column likely represents the opening price of a stock.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+-----+------------+\n",
            "|Ticker|ParsedDate| Open|PreviousOpen|\n",
            "+------+----------+-----+------------+\n",
            "|  AAPL|2018-05-31|46.81|        NULL|\n",
            "|  AAPL|2018-06-01| 47.0|       46.81|\n",
            "|  AAPL|2018-06-04|47.91|        47.0|\n",
            "|  AAPL|2018-06-05|48.27|       47.91|\n",
            "|  AAPL|2018-06-06|48.41|       48.27|\n",
            "|  AAPL|2018-06-07|48.54|       48.41|\n",
            "|  AAPL|2018-06-08|47.79|       48.54|\n",
            "|  AAPL|2018-06-11|47.84|       47.79|\n",
            "|  AAPL|2018-06-12|47.85|       47.84|\n",
            "|  AAPL|2018-06-13|48.11|       47.85|\n",
            "|  AAPL|2018-06-14|47.89|       48.11|\n",
            "|  AAPL|2018-06-15|47.51|       47.89|\n",
            "|  AAPL|2018-06-18|46.97|       47.51|\n",
            "|  AAPL|2018-06-19|46.29|       46.97|\n",
            "|  AAPL|2018-06-20|46.59|       46.29|\n",
            "|  AAPL|2018-06-21|46.81|       46.59|\n",
            "|  AAPL|2018-06-22|46.53|       46.81|\n",
            "|  AAPL|2018-06-25|45.85|       46.53|\n",
            "|  AAPL|2018-06-26|45.75|       45.85|\n",
            "|  AAPL|2018-06-27|46.31|       45.75|\n",
            "+------+----------+-----+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lag1Day = Window.partitionBy(\"Ticker\").orderBy(\"ParsedDate\")\n",
        "snapshot.withColumn(\"PreviousOpen\", func.lag(\"Open\", 1).over(lag1Day)).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In summary, this code adds a new column to the snapshot DataFrame that contains a 51-period moving average of the opening stock prices, calculated separately for each stock based on its ticker. This is a typical analysis technique in financial data processing, especially useful for identifying trends in stock prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+-----+-----+\n",
            "|Ticker|ParsedDate| Open| MA50|\n",
            "+------+----------+-----+-----+\n",
            "|  AAPL|2018-05-31|46.81|46.81|\n",
            "|  AAPL|2018-06-01| 47.0|46.91|\n",
            "|  AAPL|2018-06-04|47.91|47.24|\n",
            "|  AAPL|2018-06-05|48.27| 47.5|\n",
            "|  AAPL|2018-06-06|48.41|47.68|\n",
            "|  AAPL|2018-06-07|48.54|47.82|\n",
            "|  AAPL|2018-06-08|47.79|47.82|\n",
            "|  AAPL|2018-06-11|47.84|47.82|\n",
            "|  AAPL|2018-06-12|47.85|47.82|\n",
            "|  AAPL|2018-06-13|48.11|47.85|\n",
            "|  AAPL|2018-06-14|47.89|47.86|\n",
            "|  AAPL|2018-06-15|47.51|47.83|\n",
            "|  AAPL|2018-06-18|46.97|47.76|\n",
            "|  AAPL|2018-06-19|46.29|47.66|\n",
            "|  AAPL|2018-06-20|46.59|47.59|\n",
            "|  AAPL|2018-06-21|46.81|47.54|\n",
            "|  AAPL|2018-06-22|46.53|47.48|\n",
            "|  AAPL|2018-06-25|45.85|47.39|\n",
            "|  AAPL|2018-06-26|45.75| 47.3|\n",
            "|  AAPL|2018-06-27|46.31|47.25|\n",
            "+------+----------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "## Calculate moving average\n",
        "movingAverage = Window.partitionBy(\"Ticker\").orderBy(\"ParsedDate\").rowsBetween(-50, 0)\n",
        "(snapshot.withColumn(\"MA50\", func.avg(\"Open\").over(movingAverage))\n",
        "         .withColumn(\"MA50\", func.round(\"MA50\", 2))).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+------+-------+\n",
            "|Ticker|ParsedDate|  Open|MaxOpen|\n",
            "+------+----------+------+-------+\n",
            "|  AAPL|2022-01-04|182.63|      1|\n",
            "|  AAPL|2021-12-13|181.12|      2|\n",
            "|  AAPL|2021-12-28|180.16|      3|\n",
            "|  AAPL|2022-01-05|179.61|      4|\n",
            "|  AAPL|2021-12-30|179.47|      5|\n",
            "|  AAPL|2021-12-29|179.33|      6|\n",
            "|  AAPL|2021-12-16|179.28|      7|\n",
            "|  AAPL|2022-03-30|178.55|      8|\n",
            "|  AAPL|2021-12-31|178.09|      9|\n",
            "|  AAPL|2022-03-31|177.84|     10|\n",
            "|  AAPL|2022-01-03|177.83|     11|\n",
            "|  AAPL|2022-04-05| 177.5|     12|\n",
            "|  AAPL|2023-05-31|177.33|     13|\n",
            "|  AAPL|2021-12-27|177.09|     14|\n",
            "|  AAPL|2023-05-30|176.96|     15|\n",
            "|  AAPL|2022-03-29|176.69|     16|\n",
            "|  AAPL|2023-05-19|176.39|     17|\n",
            "|  AAPL|2022-01-12|176.12|     18|\n",
            "|  AAPL|2022-02-09|176.05|     19|\n",
            "|  AAPL|2021-12-23|175.85|     20|\n",
            "+------+----------+------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "maximumStock = Window.partitionBy(\"Ticker\").orderBy(snapshot.Open.desc())\n",
        "snapshot.withColumn(\"MaxOpen\", func.row_number().over(maximumStock)).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+------+-------+\n",
            "|Ticker|ParsedDate|  Open|MaxOpen|\n",
            "+------+----------+------+-------+\n",
            "|  AAPL|2022-01-04|182.63|      1|\n",
            "|  AAPL|2021-12-13|181.12|      2|\n",
            "|  AAPL|2021-12-28|180.16|      3|\n",
            "|  AAPL|2022-01-05|179.61|      4|\n",
            "|  AAPL|2021-12-30|179.47|      5|\n",
            "|  AMZN|2021-07-12| 187.2|      1|\n",
            "|  AMZN|2021-07-09|186.13|      2|\n",
            "|  AMZN|2021-07-07|185.87|      3|\n",
            "|  AMZN|2021-11-19|185.63|      4|\n",
            "|  AMZN|2021-07-14|185.44|      5|\n",
            "| BRK-B|2022-03-29|361.39|      1|\n",
            "| BRK-B|2022-03-28|360.59|      2|\n",
            "| BRK-B|2022-03-31| 359.0|      3|\n",
            "| BRK-B|2022-03-30|354.66|      4|\n",
            "| BRK-B|2022-03-25| 353.9|      5|\n",
            "| GOOGL|2022-02-02|151.25|      1|\n",
            "| GOOGL|2021-11-19|149.98|      2|\n",
            "| GOOGL|2021-11-08|149.83|      3|\n",
            "| GOOGL|2021-11-22|149.33|      4|\n",
            "| GOOGL|2021-11-09|149.23|      5|\n",
            "+------+----------+------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Calculate top 5 highest close price for each stock in a year\n",
        "snapshot.withColumn(\"MaxOpen\", func.row_number().over(maximumStock)).filter(\"MaxOpen<=5\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+-----+-----+\n",
            "|Ticker|ParsedDate| Open| MA50|\n",
            "+------+----------+-----+-----+\n",
            "|  AAPL|2018-05-31|46.81|46.81|\n",
            "|  AAPL|2018-06-01| 47.0|46.91|\n",
            "|  AAPL|2018-06-04|47.91|47.24|\n",
            "|  AAPL|2018-06-05|48.27| 47.5|\n",
            "|  AAPL|2018-06-06|48.41|47.68|\n",
            "|  AAPL|2018-06-07|48.54|47.82|\n",
            "|  AAPL|2018-06-08|47.79|47.82|\n",
            "|  AAPL|2018-06-11|47.84|47.82|\n",
            "|  AAPL|2018-06-12|47.85|47.82|\n",
            "|  AAPL|2018-06-13|48.11|47.85|\n",
            "|  AAPL|2018-06-14|47.89|47.86|\n",
            "|  AAPL|2018-06-15|47.51|47.83|\n",
            "|  AAPL|2018-06-18|46.97|47.76|\n",
            "|  AAPL|2018-06-19|46.29|47.66|\n",
            "|  AAPL|2018-06-20|46.59|47.59|\n",
            "|  AAPL|2018-06-21|46.81|47.54|\n",
            "|  AAPL|2018-06-22|46.53|47.48|\n",
            "|  AAPL|2018-06-25|45.85|47.39|\n",
            "|  AAPL|2018-06-26|45.75| 47.3|\n",
            "|  AAPL|2018-06-27|46.31|47.25|\n",
            "+------+----------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result = snapshot.withColumn(\"MaxOpen\", func.row_number().over(maximumStock)).filter(\"MaxOpen<=5\")\n",
        "moving_avg = (snapshot.withColumn(\"MA50\", func.avg(\"Open\").over(movingAverage))\n",
        "         .withColumn(\"MA50\", func.round(\"MA50\", 2)))\n",
        "moving_avg.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/suriarasai/BEAD2024/blob/main/colab/06_Ingestion_and_Data_Formats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "PySpark provides interface used to load DataFrame from external storage systems. We will learn how to read different data format files into DataFrame and write DataFrame back to different data format files using PySpark examples. Lastly, we will learn how to transfer data between JVM and Python processes using Apache Arrow efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# install pyspark using pip\n",
        "!pip install --ignore-install -q pyspark\n",
        "# install findspark using pip\n",
        "!pip install --ignore-install -q findspark\n",
        "\n",
        "#from pyspark import SparkConf,SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "import collections\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Ingestion\").config('spark.ui.port', '4050').getOrCreate()\n",
        "\n",
        "df_csv = spark.read.format('csv') \\\n",
        "                .option(\"inferSchema\",\"true\") \\\n",
        "                .option(\"header\",\"true\") \\\n",
        "                .option(\"sep\",\";\") \\\n",
        "                .load(\"/content/drive/MyDrive/data/DataFormat/people.csv\")\n",
        "\n",
        "df_json = spark.read.format('json') \\\n",
        "                .option(\"inferSchema\",\"true\") \\\n",
        "                .option(\"header\",\"true\") \\\n",
        "                .option(\"sep\",\";\") \\\n",
        "                .load(\"/content/drive/MyDrive/data/DataFormat/people.json\")\n",
        "\n",
        "peopleDF = spark.read.json(\"/content/drive/MyDrive/data/DataFormat/people.json\")\n",
        "\n",
        "import pyarrow.csv as pv\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "# read hdb resale price\n",
        "hdb_table = pv.read_csv(\"/content/drive/MyDrive/data/DataFormat/resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.csv\")\n",
        "# convert the CSV file to a Parquet file\n",
        "pq.write_table(hdb_table,'resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.parquet')\n",
        "hdb_parquet = pq.ParquetFile('resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.parquet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Read CSV file\n",
        "PySpark provides DataFrameReader to load a DataFrame from external storage systems (e.g. file systems, key-value stores, etc). Use SparkSession.read to access this. You can use format(source) to specify the input data source format.  \n",
        "Using csv(\"path\") or format(\"csv\").load(\"path\") of DataFrameReader, you can read a CSV file into a PySpark DataFrame, These methods take a file path to read from as an argument. When you use format(\"csv\") method, you can also specify the data sources by their fully qualified name, but for built-in sources, you can simply use their short names (csv,json, parquet, jdbc, text e.t.c).\n",
        "In this example, it shows how to read a single CSV file ‚Äúpeople.csv‚Äù into DataFrame as well as how to use your own defined schema when read file into DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+---+---------+\n",
            "| name|age|      job|\n",
            "+-----+---+---------+\n",
            "|Jorge| 30|Developer|\n",
            "|  Bob| 32|Developer|\n",
            "+-----+---+---------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read CSV file people.csv\n",
        "\n",
        "\n",
        "# Show result\n",
        "df.show()\n",
        "# Print schema\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Write csv file\n",
        "# df = spark.read.format('csv') \\\n",
        "#                 .option(\"inferSchema\",\"true\") \\\n",
        "#                 .option(\"header\",\"true\") \\\n",
        "#                 .option(\"sep\",\";\") \\\n",
        "#                 .load(\"people.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DataFrames can be saved as Parquet files, maintaining the schema information.\n",
        "peopleDF.write.format(\"parquet\").mode(\"overwrite\").save(\"people.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+\n",
            "|  name|\n",
            "+------+\n",
            "|Justin|\n",
            "+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read in the Parquet file created above.\n",
        "# Parquet files are self-describing so the schema is preserved.\n",
        "# The result of loading a parquet file is also a DataFrame.\n",
        "parquetFile = spark.read.parquet(\"people.parquet\")\n",
        "\n",
        "# Parquet files can also be used to create a temporary view and then used in SQL statements.\n",
        "parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
        "teenagers = spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\")\n",
        "teenagers.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pyarrow._parquet.FileMetaData object at 0x7b44c67af650>\n",
            "  created_by: parquet-cpp-arrow version 10.0.1\n",
            "  num_columns: 10\n",
            "  num_rows: 52203\n",
            "  num_row_groups: 1\n",
            "  format_version: 2.6\n",
            "  serialized_size: 2079\n",
            "<pyarrow._parquet.RowGroupMetaData object at 0x7b44ac0bcc20>\n",
            "  num_columns: 10\n",
            "  num_rows: 52203\n",
            "  total_byte_size: 431095\n",
            "<pyarrow._parquet.Statistics object at 0x7b44c59787c0>\n",
            "  has_min_max: True\n",
            "  min: 195000.0\n",
            "  max: 1088888.0\n",
            "  null_count: 0\n",
            "  distinct_count: 0\n",
            "  num_values: 52203\n",
            "  physical_type: DOUBLE\n",
            "  logical_type: None\n",
            "  converted_type (legacy): NONE\n"
          ]
        }
      ],
      "source": [
        "# inspect the parquet metadata\n",
        "print(hdb_parquet.metadata)\n",
        "# inspect the parquet row group metadata\n",
        "print(hdb_parquet.metadata.row_group(0))\n",
        "# inspect the column chunk metadata\n",
        "print(hdb_parquet.metadata.row_group(0).column(9).statistics)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPN5Ns80CoqczWeyFE6RBnv",
      "include_colab_link": true,
      "mount_file_id": "1CWLfwBSfIFai4_aEkQ7qYT8H5ENZIN0O",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
